{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference: Classical and Bayesian\n",
    "\n",
    "G. Richards \n",
    "(2016, 2018, 2020),\n",
    "with material from Ivezic [Sections 4.0, 4.1, 4.2, 4.3, 4.5, 5.0, 5.1, 5.2], Bevington, and Leighly.\n",
    "\n",
    "Statistical *inference* is about drawing conclusions from data, specifically determining the properties of a population by data sampling.  \n",
    "\n",
    "Three examples of inference are:\n",
    "* What is the best estimate for a model parameter\n",
    "* How confident we are about our result\n",
    "* Are the data consistent with a particular model/hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Terminology\n",
    "\n",
    "* We typically study the properties of some ***population*** by measuring ***samples*** from that population. \n",
    "* To conclude something about the population from the sample, we develop ***estimators***. An estimator is a statistic based on observed data.\n",
    "* A ***statistic*** is any function of the sample. For example, the sample mean is a statistic. But also, \"the value of the first measurement\" is also a statistic.\n",
    "* There are ***point*** and ***interval estimators***. Point estimators yield single-valued results (example: the position of an object), while with an interval estimator, the result would be a range of plausible values (example: confidence interval for the position of an object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian vs Frequentist (Ivezic 4.1)\n",
    "\n",
    "This is the point where we are supposed to have a long discussion about the various pros and cons of the two most common ways of approaching inference problems:\n",
    "* Classical (frequentist) and\n",
    "* Bayesian.\n",
    "\n",
    "Personally I don't see the need for a lengthy discussion on this.  In short, classical (frequentist) statistics is concerned with the frequency with which $A$  happens in identical repeats of an experiment, i.e., $p(A)$. Bayesian statistics is concerned instead with $p(A|B)$, which is how plausible it is for $A$ to happen given the knowledge that $B$ has happened (or is true). \n",
    "\n",
    "For more insight see [Jake VanderPlas's blog \"Frequentism and Bayesianism: A Practical Introduction](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "My colleague, Karen Leighly, dug up the following article, which might help one to understand the differences in these approaches in a relatively simply way.  The first 4 sections are what is relevant here.\n",
    "[Efron 1978](http://www.jstor.org/stable/2321163?seq=1#page_scan_tab_contents)\n",
    "\n",
    "I'll briefly (and perhaps too cavalierly) summarize it.\n",
    "\n",
    "Let's say that you get the results of an IQ test.  Any given test result might not give you your \"real\" IQ.  But it gives us a way to *estimate* it (and the possible range of values).  \n",
    "\n",
    "For a frequentist, the best estimator is just the average of many test results.  So, if you took 5 IQ tests and got a 160, then that would be the estimator of your true IQ.\n",
    "\n",
    "On the other hand, a Bayesian would say: \"but wait, I know that IQ tests are normed to 100 with a standard deviation of 15 points\".  So they will use that as \"prior\" information, which is important here since 160 is a 4$\\sigma$ outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=10, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distributions to be plotted\n",
    "sigma_values = [15, 6.7, 1]\n",
    "linestyles = ['--', '-', ':']\n",
    "mu_values = [100, 148, 160]\n",
    "labeltext = ['prior dist.', 'posterior dist.', 'observed mean']\n",
    "xplot = np.linspace(50, 200, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "\n",
    "for sigma, ls, mu, lab in zip(sigma_values, linestyles, mu_values, labeltext):\n",
    "    # create a gaussian / normal distribution\n",
    "    dist = norm(mu, sigma)\n",
    "\n",
    "    if (sigma>1):\n",
    "        plt.plot(xplot, dist.pdf(xplot), ls=ls, c='black',label=r'%s $\\mu=%i,\\ \\sigma=%.1f$' % (lab, mu, sigma))\n",
    "    else:\n",
    "        plt.plot([159.9,160.1],[0,0.8], ls=ls, color='k', label=r'%s $\\mu=%i$' % (lab, mu))\n",
    "        \n",
    "plt.xlim(50, 200)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The end result (skipping over the detailed math) is that the Bayesian estimate of the IQ is not 160, but rather 148, or more specifically that $p(141.3\\le \\mu \\le 154.7 \\, | \\, \\overline{x}=160) = 0.683$.\n",
    "\n",
    "That's actually fine, where the controvery comes in is when the Bayesian wants to do the same things but doesn't actually know the prior distribution, or when the parameter is fixed but we are trying to experimentally verify it (e.g., the speed of light)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE), Ivezic 4.2\n",
    "\n",
    "Let's not worry about classical vs. Bayesian right now and talk about maximum likelihood estimation (Ivezic, 4.2), which is relevant to both.\n",
    "\n",
    "If we know the distribution from which our data were drawn, then we can compute the **probability** or **likelihood** of our data.  \n",
    "\n",
    "For example if you know that your data are drawn from a model with a Gaussian distribution, then we've already seen that the probablity of getting a specific value of $x$ is given by\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "If we want to know the total likelihood of our *entire* data set (as opposed to one measurement) then we must compute the *product* of all the individual probabilities:\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^n p(x_i|M(\\theta)),$$\n",
    "where $M$ refers to the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can be multi-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In words, this is *the probability of the data given the model*.  However, note that while the components of $L$ may be normalized pdfs, their product is not.  Also the product can be very small, so we often take the log of $L$.\n",
    "\n",
    "We can write this out as\n",
    "$$L = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "and simplify to\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the product of the exponentials as the exponential of the sum of the arguments, which will make things easier to deal with later.\n",
    "\n",
    "That is, we have done this: $$\\prod_{i=1}^n A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_n) \\exp[-(B_i+B_{i+1}+\\ldots+B_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g.,, doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, maximizing the likelihood is the same as minimizing $\\chi^2$.  In both cases we are finding the most likely values of our model parameters (here $\\mu$ and $\\sigma$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's [an animation of linear least squares fitting](https://yihui.org/animation/example/least-squares/).\n",
    "\n",
    "They are trying to fit a line to some data.  They start by fixing the interecept and then trying 50 different values of the slope.  The red dashed lines show the difference between the predicted value and the actual value.  These are squared and summed (residual sum of squares, or $\\chi^2$) and plotted as the y axis in the right hand plot.  You see $\\chi^2$ going down as the slope changes, bottoming out at the best slope (presumably with $\\chi^2 \\sim 1$, but we can't tell from the scale shown).  Then $\\chi^2$ goes back up after we have passed through the best slope.\n",
    "\n",
    "With the best slope determined, we then try different values of the intercept, choosing the one that minimizes $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters), then MLE is about varying the parameters until we find the maximal value of $L$ (i.e., the **maximum likelihood**).  Those model parameters will also minimize $\\chi^2$.  Simple as that.  Stop and make sure that you understand this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple example would be the likelihood of rolling a certain combination of numbers on a six-sided die.\n",
    "The probability of rolling a 3 is $1/6$ (as is the probability of *any* roll).  So, what is the probability of rolling (in no particular order): {1,1,2,3,3,3,4,5,6,6}?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print((1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6))\n",
    "print((1./6)**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, even for 10 rolls of the die, the likelihood is pretty small.  That's just because there are *lots* of possible combinations of rolling a die 10 times.  This particular series of numbers is just as likely as any other.  \n",
    "\n",
    "Students who took PHYS 114 with me will recall that the result is related to the number of *combinations* ($n$ choose $r$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Write some code to compute the probability for N rolls\n",
    "import numpy as np\n",
    "N=____ #Number of rolls\n",
    "L=____ #Likelihood, initialize to unity\n",
    "rolls = np.array([])\n",
    "for i in np.arange(____): #Loop over each roll\n",
    "    #Append a single new roll to \"rolls\" between 1 and 6 (careful) to the rolls array\n",
    "    rolls = np.append(____,np.random.randint(low=____,high=____,size=____))\n",
    "    L = L*(1./6) #The likelihood of each roll is 1/6\n",
    "print(L,rolls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLE applied to a Homoscedastic Gaussian (Ivezic 4.2.3)\n",
    "\n",
    "Let's take a look at an example using a Gaussian model where all the measurements have the same error ($\\sigma$).  This is known as having **homoscedastic** errors.  Don't be intimidated by the fancy word, statisticians just like to sound smart, so they say \"homoscedastic\" instead of \"uniform errors\".  Later we will consider the case where the measurements can have different errors ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an experiment with data $D=\\{x_i\\}$ in 1D with Gaussian errors, we have\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement.\n",
    "\n",
    "If $\\sigma$ is both uniform and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the *log-likelihood function* as ${\\rm lnL} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have $${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\mu_0} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\mu_o)}{\\sigma^2} = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$ (at least in this case), that says \n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\mu_0 = N \\mu_0.$$\n",
    "\n",
    "Thus we find that\n",
    "$$\\mu_0 = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which is just the arithmetic mean of all the measurements.\n",
    "\n",
    "As promised last week, that's where the formula that you know and love for the mean comes from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Sample Mean is an ML Estimator\n",
    "\n",
    "So the sample mean ($\\overline{x} = \\mu_0$) of observations drawn from a $\\mathscr{N}(\\mu, \\sigma=const)$ distribution is a maximum-likelihood estimator of the distribution's $\\mu$ parameter.\n",
    "\n",
    "We'd intuitively guess that, but this derivation clarifies our choice: as an estimator of the **real** value of $\\mu$, we adopt the value $\\mu_0$ for which the data set is maximally likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we just discussed, in an experiment with data $D=\\{x_i\\}$ in 1D with Gaussian errors, we have\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Let's create some data and see what the resulting likelihood looks like for some example points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Load up the algorithms we are going to need.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to draw a homoscedastic sample of ${x_i}$ from a Gaussian and compute the likelihood.\n",
    "\n",
    "First generate a sample of `N=3` points drawn from a normal distribution with `mu=1.0` and `sigma=0.2`: $\\mathscr{N}(\\mu,\\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = ___ #Complete\n",
    "mu = ___\n",
    "sigma = ___ \n",
    "np.random.seed(42)\n",
    "sample = norm(___,___).rvs(___)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think back to Lecture 2 when we did a Kernel Density Estimate.  Treat each of those observations as an estimate of the true distribution.  So, we are going to center a Gaussian (with the knownn $\\sigma$) at each point, this is the likelihood $p(x_i|\\mu,\\sigma)$.\n",
    "\n",
    "Plot each of the likelihoods separately.  Also plot their product.  Make the $x$ axis a grid of 1000 points uniformly sampled between $x=0$ and $x=2$.\n",
    "\n",
    "Note that, according to [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), `norm.pdf(x, loc, scale)` is identically equivalent to `norm.pdf(y)/scale` with `y=(x-loc)/scale$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make the plot and see if you get the same as me.\n",
    "xgrid = np.linspace(___,___,___)\n",
    "L1 = norm.pdf(___,loc=___,scale=___) #This is a Gaussian PDF sampled uniformly, centered at a specific location.\n",
    "L2 = norm.pdf(___,loc=___,scale=___)\n",
    "L3 = norm.pdf(___,loc=___,scale=___)\n",
    "L = ___ #Total L is ???\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(____, ____, ls='-', c='green', label=r'$L(x_1)$')\n",
    "plt.plot(____, ____, ls='-', c='red', label=r'$L(x_2)$')\n",
    "plt.plot(____, ____, ls='-', c='blue', label=r'$L(x_3)$')\n",
    "plt.plot(____, ____, ls='-', c='black', label=r'$L(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {
    "MLEexample.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You should get something that looks like this:\n",
    "![MLEexample.jpg](attachment:MLEexample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can just read off the maximum likelihood solution.  Use `np.argsort()` to figure out the argument of the largest value and print that index of `xgrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "indices = ___\n",
    "index_max = indices[___]\n",
    "print(\"Likelihood is maximized at %.3f\" % xgrid[index_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantifying Estimated Uncertainty (Ivezic 4.2.5)\n",
    "\n",
    "Our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the likelihood function, but we'd like to quantify it with a few numbers.\n",
    "\n",
    "We *define* the uncertainty on our MLEs as second (partial) derivatives of log-likelihood:\n",
    "\n",
    "$$\\sigma_{jk} = \\left( - \\frac{d^2}{d\\theta_j} \\frac{\\ln L}{d\\theta_k} \\Biggr\\rvert_{\\theta=\\theta_0}\\right)^{-1/2}.$$\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$, of this **covariance matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, the uncertainly on the mean is \n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "$$\\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then $$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is the result that we are already familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "The $(\\mu_0 - \\sigma_\\mu, \\mu_0 + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find that 68 experiments yield a result within the computed confidence interval ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence Estimates: Bootstrap and Jackknife (Ivezic 4.5)\n",
    "\n",
    "We often assume that the distribution is Gaussian and our samples are large, but even if that is not the case, we can still compute good confidence intervals (e.g., $a<x<b$ with 95\\% confidence) using *resampling* strategies.\n",
    "\n",
    "Remember that we have a data set $\\{x_i\\}$ from which we have estimated the distribution as $f(x)$ for a true distribution $h(x)$.  \n",
    "\n",
    "In **bootstrapping** we map the uncertainty of the parameters by re-sampling from our distribution (with replacement) $B$ times, such that we obtain $B$ measures of our parameters.   So, if we have $i=1,\\dots,N$ data points in $\\{x_i\\}$, we draw $N$ of them to make a new sample, where some values of $\\{x_i\\}$ will be used more than once.\n",
    "\n",
    "The **jackknife** method is similar except that we don't use a sample size of $N$, rather we leave off one or more of the observations from $\\{x_i\\}$.  As with bootstrap, we do this multiple times, generating samples from which we can determine our uncertainties.\n",
    "\n",
    "It is generally a good idea to use both methods and compare the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example of bootstrap is given below, using [astroML.resample.bootstrap](http://www.astroml.org/modules/generated/astroML.resample.bootstrap.html), where the arguments are 1) the data, 2) the number of bootstrap resamples to use, and 3) the statistic to be computed.\n",
    "\n",
    "You'll get some more practice with this in a homework assignment based on Data Camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 4.3, modified slightly by GTR\n",
    "# %load ../code/fig_bootstrap_gaussian.py\n",
    "\"\"\"\n",
    "Bootstrap Calculations of Error on Mean\n",
    "---------------------------------------\n",
    "Figure 4.3.\n",
    "\n",
    "The bootstrap uncertainty estimates for the sample standard deviation\n",
    ":math:`\\sigma` (dashed line; see eq. 3.32) and :math:`\\sigma_G` (solid line;\n",
    "see eq. 3.36). The sample consists of N = 1000 values drawn from a Gaussian\n",
    "distribution with :math:`\\mu = 0` and :math:`\\sigma = 1`. The bootstrap\n",
    "estimates are based on 10,000 samples. The thin lines show Gaussians with\n",
    "the widths determined as :math:`s / \\sqrt{2(N - 1)}` (eq. 3.35) for\n",
    ":math:`\\sigma` and :math:`1.06 s / \\sqrt{N}` (eq. 3.37) for :math:`\\sigma_G`.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.resample import bootstrap\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "m = 1000  # number of points\n",
    "n = 10000  # number of bootstraps\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# sample values from a normal distribution\n",
    "np.random.seed(123)\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute bootstrap resamplings of data\n",
    "mu1_bootstrap = bootstrap(data, n, np.std, kwargs=dict(axis=1, ddof=1))\n",
    "mu2_bootstrap = bootstrap(data, n, sigmaG, kwargs=dict(axis=1))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the theoretical expectations for the two distributions\n",
    "xgrid = np.linspace(0.8, 1.2, 1000)\n",
    "\n",
    "sigma1 = 1. / np.sqrt(2 * (m - 1))\n",
    "pdf1 = norm(1, sigma1).pdf(xgrid)\n",
    "\n",
    "sigma2 = 1.06 / np.sqrt(m)\n",
    "pdf2 = norm(1, sigma2).pdf(xgrid)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.hist(mu1_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std. dev.)}$')\n",
    "ax.plot(xgrid, pdf1, color='gray')\n",
    "\n",
    "ax.hist(mu2_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$')\n",
    "ax.plot(xgrid, pdf2, color='gray')\n",
    "\n",
    "ax.set_xlim(0.82, 1.18)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$')\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Ivezic: \"The bootstrap uncertainty estimates for the sample standard deviation $\\sigma$ (dashed line; see Eq. 3.32) and $\\sigma_G$ (solid line; see Eq. 3.36). The sample consists of N = 1000 values drawn from a Gaussian distribution with $\\mu = 0$ and $\\sigma = 1$. The bootstrap estimates are based on 10,000 samples. The thin grey lines show Gaussians with the widths determined as $s / \\sqrt{2(N - 1)}$ (Eq. 3.35) for $\\sigma$ and $1.06 s / \\sqrt{N}$ (Eq. 3.37) for $\\sigma_G$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLE applied to a Heteroscedastic Gaussian (Ivezic 4.2.6)\n",
    "\n",
    "Now let's look at a case where the errors are heteroscedastic.  For example if we are measuring the length of a [rod](https://www.nist.gov/image/meter27jpg) and have $N$ measurements, $\\{x_i\\}$, where the error for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is **not** a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\mu_0} = \\sum_{i=1}^N \\frac{(x_i - \\mu_o)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\mu_o}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\mu_0 = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "which is just a variance-weighted mean, with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost Functions\n",
    "\n",
    "Recall from Lecture 3 that we measured the deviation from expected in three different ways\n",
    "$$d_i = x_i - \\mu,$$  \n",
    "$$|x_i-\\mu|,$$\n",
    "and\n",
    "$$(x_i-\\mu)^2.$$\n",
    "\n",
    "When doing trying to determine the best model parameters, we have to specify a \"cost function\", which is basically the prescription for evaluating the difference (distance) between our estimator and the true value.  This is otherwise known as the \"norm\", or the total length of the distances.  The first form above represents the $L_0$ norm, while the next two are the $L_1$ norm (the \"taxi-cab\" norm) and the $L_2$ norm (the \"as the crow flies\" norm).  \n",
    "\n",
    "So far we have been using an $L_2$ norm (which comes about simply from the definition of a Gaussian with the $(x_i-\\mu)^2$ term).  Later in the course we will encounter machine learning algorithms that allow us to specify different norms (different cost functions).\n",
    "\n",
    "See [this Medium article](https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c) and Ivezic, 4.2.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Truncated/Censored Data\n",
    "\n",
    "Note that knowing how to deal with missing data points (\"censored data\") is often quite important, but adds complications that we don't have time to get into here.  For more, see Ivezic 4.2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Goodness\" of Fit (Ivezic 4.3)\n",
    "\n",
    "The MLE approach tells us what the \"best\" model parameters are, but not how good the fit actually is.  If the model is wrong, \"best\" might not be particularly revealing!  For example, if you have $N$ points drawn from a linear distribution, you can always fit the data perfectly with an $N-1$ order polynomial.  But that won't help you predict future measurements.\n",
    "\n",
    "We can describe the **goodness of fit** in words simply as whether or not it is likely to have obtained $\\ln L_0$ by randomly drawing from the data.  That means that we need to know the *distribution* of $\\ln L$ and not just the maximum.  \n",
    "\n",
    "For the Gaussian case we have just described, we do a standard transform of variables and compute the so-called $z$ score for each data point (basically the number of standard deviations away from the mean that this point is), writing \n",
    "$$z_i = (x_i-\\mu)/\\sigma,$$ then\n",
    "$$\\ln L = {\\rm constant} - \\frac{1}{2}\\sum_{i=1}^N z^2 = {\\rm constant} - \\frac{1}{2}\\chi^2.$$\n",
    "\n",
    "Here, $\\chi^2$ is the thing whose distribution we discussed last week.\n",
    "\n",
    "So $\\ln L$ is distributed as $\\chi^2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to make the plot\n",
    "# %load ../code/fig_chi2_distribution.py\n",
    "\"\"\"\n",
    "Example of a chi-squared distribution\n",
    "---------------------------------------\n",
    "Figure 3.14.\n",
    "\n",
    "This shows an example of a :math:`\\chi^2` distribution with various parameters.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "k_values = [1, 2, 5, 7]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "mu = 0\n",
    "xplot = np.linspace(-1, 10, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "\n",
    "for k, ls in zip(k_values, linestyles):\n",
    "    dist = chi2(k, mu)\n",
    "    idx = np.argsort(dist.pdf(xplot))[-1]\n",
    "    #print(f\"The peak Q value for {k} degrees of freedom is {xplot[idx]}.\")\n",
    "    print(\"The peak Q value for {0:d} degrees of freedom is {1:3.2f}.\".format(k,xplot[idx]))\n",
    "                                                                                       \n",
    "    plt.plot(xplot, dist.pdf(xplot), ls=ls, c='black',\n",
    "             label=r'$k=%i$' % k)\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$Q$')\n",
    "plt.ylabel(r'$p(Q|k)$')\n",
    "plt.title(r'$\\chi^2\\ \\mathrm{Distribution}$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We define the $\\chi^2$ per degree of freedom, $\\chi^2_{dof}$, as\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-k}\\sum_{i=1}^N z^2_i.$$\n",
    "\n",
    "For a good fit, we would expect that $\\chi^2_{dof}\\approx 1$.  If $\\chi^2_{dof}$ is significantly larger than 1, then it is likely that we are not using the correct model.\n",
    "\n",
    "We can also get overly high or low values of $\\chi^2_{dof}$ if our errors are under- or over-estimated as shown below:\n",
    "\n",
    "![Ivezic, Figure 4.1](http://www.astroml.org/_images/fig_chi2_eval_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Statistical Inference (Ivezic 5.0, 5.1)\n",
    "\n",
    "Up to now in this lecture we have been computing the **likelihood** $p(D|M)$.  In Bayesian inference, we instead evaluate the **posterior probability** taking into account **prior** information.\n",
    "\n",
    "Recall from the BasicStats lecture that Bayes' Rule is:\n",
    "$$p(M|D) = \\frac{p(D|M)p(M)}{p(D)},$$\n",
    "where $D$ is for data and $M$ is for model.\n",
    "\n",
    "We wrote this in words as:\n",
    "$${\\rm Posterior Probability} = \\frac{{\\rm Likelihood}\\times{\\rm Prior}}{{\\rm Evidence}}.$$\n",
    "\n",
    "If we explicitly recognize prior information, $I$, and the model parameters, $\\theta$, then we can write:\n",
    "$$p(M,\\theta|D,I) = \\frac{p(D|M,\\theta,I)p(M,\\theta|I)}{p(D|I)},$$\n",
    "where we can omit the explict dependence on $\\theta$ by writing $M$ instead of $M,\\theta$ where appropriate.  \n",
    "\n",
    "Note that it is often that case that $p(D|I)$ is not evaluated explictly since the likelihood can be normalized such that the \"evidence\" is unity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Bayesian Statistical Inference process is then\n",
    "* formulate the likelihood, $p(D|M,\\theta,I)$, which is what we have been talking about so far today\n",
    "* chose a prior, $p(M,\\theta|I)$, which incorporates other information beyond the data in $D$\n",
    "* determine the posterior pdf, $p(M,\\theta|D,I)$\n",
    "* search for the model paramters that maximize the posterior pdf\n",
    "* quantify the uncertainty of the model parameter estimates\n",
    "* test the hypothesis being addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does our answer change for our earlier example if we include a Bayesian prior (like we assumed for the IQ problem) and instead maximize the posterior probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Things that we have already done above, but need to reset here\n",
    "N = 3 #Complete\n",
    "mu = 1.0\n",
    "sigma = 0.2 \n",
    "np.random.seed(42)\n",
    "sample = norm(mu,sigma).rvs(N)\n",
    "\n",
    "xgrid = np.linspace(0,2,1000)\n",
    "L1 = norm.pdf(xgrid,loc=sample[0],scale=sigma)\n",
    "L2 = norm.pdf(xgrid,loc=sample[1],scale=sigma)\n",
    "L3 = norm.pdf(xgrid,loc=sample[2],scale=sigma)\n",
    "\n",
    "#New things\n",
    "Prior = norm.pdf(___,loc=___,scale=___) #Prior PDF\n",
    "Post1 = ___*___ #Posterior PDF for the first measurement\n",
    "Post2 = ___*___\n",
    "Post3 = ___*___\n",
    "Post = ___ #Total posterior PDF for all the measurements\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, Post1, ls='-', c='green', label=r'$P(x_1)$')\n",
    "plt.plot(xgrid, ____, ls='-', c='red', label=r'$P(x_2)$')\n",
    "plt.plot(xgrid, ____, ls='-', c='blue', label=r'$P(x_3)$')\n",
    "plt.plot(xgrid, Post/5, ls='-', c='black', label=r'$P(\\{x\\})$') #Scaled for the sake of display\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 10.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(\\mu,\\sigma|x_i)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx = np.argsort(Post)\n",
    "print(\"Posterior PDF is maximized at %.3f\" % xgrid[idx[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See what happens when you have just 2 measurements, but one has a much larger error than the other (i.e., the errors are heteroscedastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "L1 = norm.pdf(xgrid,loc=___,scale=___) #Measurement with small error\n",
    "L2 = norm.pdf(xgrid,loc=___,scale=___) #Measurement with large error (give it a very different location parameter)\n",
    "L = L1 * L2 \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, L1, ls='-', c='green', label=r'$L(x_1): good$')\n",
    "plt.plot(xgrid, L2, ls='-', c='red', label=r'$L(x_2): lousy$')\n",
    "plt.plot(xgrid, L, ls='-', c='black', label=r'$L(\\{x\\}): weighted$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 9.5)\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$')\n",
    "plt.title('Weighted measurements (not normalized)')\n",
    "plt.legend()\n",
    "\n",
    "idx = np.argsort(L)\n",
    "print(\"Likelihood is maximized at %.3f\" % xgrid[idx[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Priors (Ivezic 5.2)\n",
    "\n",
    "Priors can be **informative** or **uninformative**.  As it sounds, informative priors are based on existing information that might be available.  Uninformative priors can be thought of as \"default\" priors, i.e., what your prior is if you weren't explicitly including a prior, e.g, a \"flat\" prior like $p(\\theta|M,I) \\propto {\\rm C}$.\n",
    "\n",
    "For the IQ test example, what kind of prior did we use?\n",
    "\n",
    "In a *hierarchical Bayesian* analysis the priors themselves can have parameters and priors (hyperparameters and hyperpriors), but let's not worry about that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While determining good priors is important for Bayesian analysis, I don't want to get distracted by it here.  You can read more about it in Ivezic, 5.2.  However, I'll briefly introduce 3 principles here.\n",
    "\n",
    "#### The Principle of Indifference\n",
    "\n",
    "Essentially this means adopting a uniform prior, though you have to be a bit careful.  Saying that an asteroid is equally likely to hit anywhere on the Earth is not the same as saying that all latitudes of impact are equally likely.  Assuming $1/6$ for a six-side die would be an example of indifference.\n",
    "\n",
    "#### The Principle of Invariance (or Consistency)\n",
    "\n",
    "This applies to location and scale invariance.  Location invariance suggests a uniform prior (within the accepted bounds).  Scale invariance gives us priors that look like $p(A|I) \\propto 1/A$.\n",
    "\n",
    "#### The Principle of Maximum Entropy\n",
    "\n",
    "The principle of maximum entropy is discussed in Ivezic, 5.2.2.\n",
    "It is often true that Bayesian analysis and traditional MLE are essentially equivalent.  However, in some cases, considering the priors can have significant consequences. \n",
    "See Ivezic $\\S$5.5 for such an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis of a Heteroscedastic Gaussian distribution with Bayesian Priors (Ivezic 5.6)\n",
    "\n",
    "Consider the case of measuring a rod as above.  We want to know the posterior pdf for the length of the rod, $p(M,\\theta|D,I) = p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$.\n",
    "\n",
    "For the likelihood we have\n",
    "$$L = p(\\{x_i\\}|\\mu,I) = \\prod_{i=1}^N \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma_i^2}\\right).$$\n",
    "\n",
    "In the Bayesian case, we also need a prior.  We'll adopt a uniform distribution given by\n",
    "$$p(\\mu|I) = C, \\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max},$$\n",
    "where $C = \\frac{1}{\\mu_{\\rm max} - \\mu_{\\rm min}}$ between the min and max and is $0$ otherwise.\n",
    "\n",
    "The log of the posterior pdf is then\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "This is exactly the same as we saw before, except that the value of the constant is different.  Since the constant doesn't come into play, we get the same result as before:\n",
    " \n",
    "$$\\mu^0 = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We get the same result because we used a flat prior.  If the case were homoscedastic instead of heteroscedastic, we obviously would get the result from our first example.\n",
    "\n",
    "Now let's consider the case where $\\sigma$ is not known, but rather needs to be determined from the data.  In that case, the posterior pdf that we seek is not $p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$, but rather $p(\\mu,\\sigma|\\{x_i\\},I)$.\n",
    "\n",
    "As before we have\n",
    "$$L = p(\\{x_i\\}|\\mu,\\sigma,I) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "except that now $\\sigma$ is uknown instead of given (meaning we need to move it to the left of the \"pipe\").\n",
    "\n",
    "Our Bayesian prior is now 2D instead of 1D and we'll adopt \n",
    "$$p(\\mu,\\sigma|I) \\propto \\frac{1}{\\sigma},\\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max} \\; {\\rm and} \\; \\sigma_{\\rm min} < \\sigma < \\sigma_{\\rm max}.$$\n",
    "\n",
    "That is, all values of $\\mu$ are equally likely (within the range indicated), but we'll down-weight the likelihood of large errors (again limiting $\\sigma$ to some range).  Note that the ranges actually drop out since they are constants.\n",
    "\n",
    "With proper normalization, we have\n",
    "$$p(\\{x_i\\}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C\\frac{1}{\\sigma^{(N+1)}}\\prod_{i=1}^N \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2}  \\right),$$\n",
    "where\n",
    "$$C = (2\\pi)^{-N/2}(\\mu_{\\rm max}-\\mu_{\\rm min})^{-1} \\left[\\ln \\left( \\frac{\\sigma_{\\rm max}}{\\sigma_{\\rm min}}\\right) \\right]^{-1}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The log of the posterior pdf is\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Right now that has $x_i$ in it, which isn't that helpful, but since we are assuming a Gaussian distribution, we can take advantage of the fact that the mean, $\\overline{x}$, and the variance, $V (=s^2)$, completely characterize the distribution.  So we can write this expression in terms of those variables instead of $x_i$.  Skipping over the math details (see Ivezic $\\S$5.6.1), we find\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\frac{N}{2\\sigma^2}\\left( (\\overline{x}-\\mu)^2 + V  \\right).$$\n",
    "\n",
    "Note that this expression only contains the 2 parameters that we are trying to determine: $(\\mu,\\sigma)$ and 3 values that we can determine directly from the data: $(N,\\overline{x},V)$.\n",
    "\n",
    "Load and execute the next cell to visualize the posterior pdf for the case of $(N,\\overline{x},V)=(10,1,4)$.  Change `usetex=True` to `usetex=False` if you have trouble with the plotting.  Try changing the values of $(N,\\overline{x},V)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# %load code/fig_likelihood_gaussian.py\n",
    "\"\"\"\n",
    "Log-likelihood for Gaussian Distribution\n",
    "----------------------------------------\n",
    "Figure5.4\n",
    "An illustration of the logarithm of the posterior probability density\n",
    "function for :math:`\\mu` and :math:`\\sigma`, :math:`L_p(\\mu,\\sigma)`\n",
    "(see eq. 5.58) for data drawn from a Gaussian distribution and N = 10, x = 1,\n",
    "and V = 4. The maximum of :math:`L_p` is renormalized to 0, and color coded as\n",
    "shown in the legend. The maximum value of :math:`L_p` is at :math:`\\mu_0 = 1.0`\n",
    "and :math:`\\sigma_0 = 1.8`. The contours enclose the regions that contain\n",
    "0.683, 0.955, and 0.997 of the cumulative (integrated) posterior probability.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "\n",
    "\n",
    "def gauss_logL(xbar, V, n, sigma, mu):\n",
    "    \"\"\"Equation 5.57: gaussian likelihood\"\"\"\n",
    "    return (-(n + 1) * np.log(sigma)\n",
    "            - 0.5 * n * ((xbar - mu) ** 2 + V) / sigma ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the grid and compute logL\n",
    "sigma = np.linspace(1, 5, 70)\n",
    "mu = np.linspace(-3, 5, 70)\n",
    "xbar = 1\n",
    "V = 4\n",
    "n = 10\n",
    "\n",
    "logL = gauss_logL(xbar, V, n, sigma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "plt.text(0.5, 0.93, r'$L(\\mu,\\sigma)\\ \\mathrm{for}\\ \\bar{x}=1,\\ V=4,\\ n=10$',\n",
    "         bbox=dict(ec='k', fc='w', alpha=0.9),\n",
    "         ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel(r'$\\sigma$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The shaded region is the posterior probability.  The contours are the confidence intervals.  We can compute those by determining the marginal distribution at each $(\\mu,\\sigma)$.  The top panels of the figures below show those marginal distributions.  The solid line is what we just computed.  The dotted line is what we would have gotten for a uniform prior--not that much difference.  The dashed line is the MLE result, which is quite different.  The bottom panels show the cumulative distribution.\n",
    "\n",
    "![Ivezic, Figure 5.5](http://www.astroml.org/_images/fig_posterior_gaussian_1.png)\n",
    "\n",
    "\n",
    "Note that the marginal pdfs follow a Student's $t$ Distribution, which becomes Gaussian for large $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "To review: the Bayesian Statistical Inference process is\n",
    "* formulate the likelihood, $p(D|M,\\theta,I)$\n",
    "* chose a prior, $p(M,\\theta|I)$, which incorporates other information beyond the data in $D$\n",
    "* determine the posterior pdf, $p(M,\\theta|D,I)$\n",
    "* search for the model paramters that maximize the posterior pdf\n",
    "* quantify the uncertainty of the model parameter estimates\n",
    "* test the hypothesis being addressed\n",
    "\n",
    "The last part we haven't talked about yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we wanted to model the mixture of a Gauassian distribution with a uniform distribution.  When might that be useful?  Well, for example:\n",
    "\n",
    "![Atlas Higgs Boson Example](https://atlas.cern/sites/atlas-public.web.cern.ch/files/Higgsmass_fig1_comb.jpg)\n",
    "\n",
    "Obviously this isn't exactly a Gaussian and a uniform distribution, but a line feature superimposed upon a background is the sort of thing that a physicist might see and is pretty close to this case for a local region around the feature of interest.  This is the example discussed in Ivezic $\\S$5.6.5.\n",
    "\n",
    "For this example, we will assume that the location parameter, $\\mu$, is known (say from theory) and that the errors in $x_i$ are negligible compared to $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The likelihood of obtaining a measurement, $x_i$, in this example can be written as\n",
    "$$L = p(x_i|A,\\mu,\\sigma,I) = \\frac{A}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right) + \\frac{1-A}{W}.$$\n",
    "\n",
    "Here the background probability is taken to be $0 < x < W$ and 0 otherwise.  The feature of interest lies between $0$ and $W$.  $A$ and $1-A$ are the relative strengths of the two components, which are obviously anti-correlated.  Note that there will be covariance between $A$ and $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we adopt a uniform prior in both $A$ and $\\sigma$:\n",
    "$$p(A,\\sigma|I) = C, \\; {\\rm for} \\; 0\\le A<A_{\\rm max} \\; {\\rm and} \\; 0 \\le \\sigma \\le \\sigma_{\\rm max},$$\n",
    "then the posterior pdf is given by\n",
    "$$\\ln [p(A,\\sigma|\\{x_i\\},\\mu,W)] = \\sum_{i=1}^N \\ln \\left[\\frac{A}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\right)  + \\frac{1-A}{W} \\right].$$\n",
    "\n",
    "The figure below (Ivezic, 5.13) shows an example for $N=200, A=0.5, \\sigma=1, \\mu=5, W=10$.  Specifically, the bottom panel is a result drawn from this distribution and the top panel is the likelihood distribution derived from the data in the bottom panel.\n",
    "![Ivezic, Figure 5.13](http://www.astroml.org/_images/fig_likelihood_gausslin_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more realistic example might be one where all three parameters are unknown: the location, the width, and the background level.  But that will have to wait until $\\S$5.8.6.\n",
    "\n",
    "In the meantime, note that we have not binned the data, $\\{x_i\\}$.  We only binned Figure 5.13 for the sake of visualizaiton.  However, sometimes the data are inherently binned (e.g., the detector is pixelated).  In that case, the data would be in the form of $(x_i,y_i)$, where $y_i$ is the number of counts at each location.  We'll skip over this example, but you can read about it in Ivezic $\\S$5.6.6.  A refresher on the Poission distribution (Ivezic $\\S$3.3.4) might be appropriate first."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
