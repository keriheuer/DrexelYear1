{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistical Inference\n",
    "\n",
    "G. Richards\n",
    "(2016, 2018, 2020)\n",
    "with input from Ivezic $\\S5$, Bevington, Karen Leighly's [Bayesian Stats](http://seminar.ouml.org/lectures/bayesian-statistics/) and [MCMC](http://seminar.ouml.org/lectures/monte-carlo-markov-chain-mcmc/) lectures, and [Thomas Wiecki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you haven't already, please pause for a few minutes and install these two packages `emcee` and `pymc3` before going through today's notebook.\n",
    "\n",
    "```\n",
    "conda install -c default -c conda-forge emcee\n",
    "conda install pymc3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis of a Heteroscedastic Gaussian distribution with Bayesian Priors (Ivezic 5.6)\n",
    "\n",
    "Consider the case of measuring a rod as above.  We want to know the posterior pdf for the length of the rod, $p(M,\\theta|D,I) = p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$.\n",
    "\n",
    "For the likelihood we have\n",
    "$$L = p(\\{x_i\\}|\\mu,I) = \\prod_{i=1}^N \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma_i^2}\\right).$$\n",
    "\n",
    "In the Bayesian case, we also need a prior.  We'll adopt a uniform distribution given by\n",
    "$$p(\\mu|I) = C, \\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max},$$\n",
    "where $C = \\frac{1}{\\mu_{\\rm max} - \\mu_{\\rm min}}$ between the min and max and is $0$ otherwise.\n",
    "\n",
    "The log of the posterior pdf is then\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "This is exactly the same as we saw before, except that the value of the constant is different.  Since the constant doesn't come into play, we get the same result as before:\n",
    " \n",
    "$$\\mu^0 = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We get the same result because we used a flat prior.  If the case were homoscedastic instead of heteroscedastic, we obviously would get the result from our first example.\n",
    "\n",
    "Now let's consider the case where $\\sigma$ is not known, but rather needs to be determined from the data.  In that case, the posterior pdf that we seek is not $p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$, but rather $p(\\mu,\\sigma|\\{x_i\\},I)$.\n",
    "\n",
    "As before we have\n",
    "$$L = p(\\{x_i\\}|\\mu,\\sigma,I) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "except that now $\\sigma$ is uknown instead of given (meaning we need to move it to the left of the \"pipe\").\n",
    "\n",
    "Our Bayesian prior is now 2D instead of 1D and we'll adopt \n",
    "$$p(\\mu,\\sigma|I) \\propto \\frac{1}{\\sigma},\\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max} \\; {\\rm and} \\; \\sigma_{\\rm min} < \\sigma < \\sigma_{\\rm max}.$$\n",
    "\n",
    "That is, all values of $\\mu$ are equally likely (within the range indicated), but we'll down-weight the likelihood of large errors (again limiting $\\sigma$ to some range).  Note that the ranges actually drop out since they are constants.\n",
    "\n",
    "With proper normalization, we have\n",
    "$$p(\\{x_i\\}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C\\frac{1}{\\sigma^{(N+1)}}\\prod_{i=1}^N \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2}  \\right),$$\n",
    "where\n",
    "$$C = (2\\pi)^{-N/2}(\\mu_{\\rm max}-\\mu_{\\rm min})^{-1} \\left[\\ln \\left( \\frac{\\sigma_{\\rm max}}{\\sigma_{\\rm min}}\\right) \\right]^{-1}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The log of the posterior pdf is\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Right now that has $x_i$ in it, which isn't that helpful, but since we are assuming a Gaussian distribution, we can take advantage of the fact that the mean, $\\overline{x}$, and the variance, $V (=s^2)$, completely characterize the distribution.  So we can write this expression in terms of those variables instead of $x_i$.  Skipping over the math details (see Ivezic $\\S$5.6.1), we find\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\frac{N}{2\\sigma^2}\\left( (\\overline{x}-\\mu)^2 + V  \\right).$$\n",
    "\n",
    "Note that this expression only contains the 2 parameters that we are trying to determine: $(\\mu,\\sigma)$ and 3 values that we can determine directly from the data: $(N,\\overline{x},V)$.\n",
    "\n",
    "Load and execute the next cell to visualize the posterior pdf for the case of $(N,\\overline{x},V)=(10,1,4)$.  Change `usetex=True` to `usetex=False` if you have trouble with the plotting.  Try changing the values of $(N,\\overline{x},V)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# %load code/fig_likelihood_gaussian.py\n",
    "\"\"\"\n",
    "Log-likelihood for Gaussian Distribution\n",
    "----------------------------------------\n",
    "Figure5.4\n",
    "An illustration of the logarithm of the posterior probability density\n",
    "function for :math:`\\mu` and :math:`\\sigma`, :math:`L_p(\\mu,\\sigma)`\n",
    "(see eq. 5.58) for data drawn from a Gaussian distribution and N = 10, x = 1,\n",
    "and V = 4. The maximum of :math:`L_p` is renormalized to 0, and color coded as\n",
    "shown in the legend. The maximum value of :math:`L_p` is at :math:`\\mu_0 = 1.0`\n",
    "and :math:`\\sigma_0 = 1.8`. The contours enclose the regions that contain\n",
    "0.683, 0.955, and 0.997 of the cumulative (integrated) posterior probability.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "\n",
    "\n",
    "def gauss_logL(xbar, V, n, sigma, mu):\n",
    "    \"\"\"Equation 5.57: gaussian likelihood\"\"\"\n",
    "    return (-(n + 1) * np.log(sigma)\n",
    "            - 0.5 * n * ((xbar - mu) ** 2 + V) / sigma ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the grid and compute logL\n",
    "sigma = np.linspace(1, 5, 70)\n",
    "mu = np.linspace(-3, 5, 70)\n",
    "\n",
    "#Change these to change the output posterior PDF\n",
    "xbar = 1 # Mean as determined from the data\n",
    "V = 4 # Variance as determined from the data\n",
    "n = 10 # Total number of data points in the sample\n",
    "\n",
    "logL = gauss_logL(xbar, V, n, sigma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "plt.text(0.5, 0.93, r'$L(\\mu,\\sigma)\\ \\mathrm{for}\\ \\bar{x}=1,\\ V=4,\\ n=10$',\n",
    "         bbox=dict(ec='k', fc='w', alpha=0.9),\n",
    "         ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel(r'$\\sigma$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The shaded region is the posterior probability.  The contours are the confidence intervals.  We can compute those by determining the marginal distribution at each $(\\mu,\\sigma)$.  The top panels of the figures below show those marginal distributions.  The solid line is what we just computed.  The dotted line is what we would have gotten for a uniform prior--not that much difference.  The dashed line is the MLE result, which is quite different.  The bottom panels show the cumulative distribution.\n",
    "\n",
    "![Ivezic, Figure 5.5](http://www.astroml.org/_images/fig_posterior_gaussian_1.png)\n",
    "\n",
    "\n",
    "Note that the marginal pdfs follow a Student's $t$ Distribution, which becomes Gaussian for large $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we wanted to model the mixture of a Gauassian distribution with a uniform distribution.  When might that be useful?  Well, for example:\n",
    "\n",
    "![Atlas Higgs Boson Example](https://atlas.cern/sites/atlas-public.web.cern.ch/files/Higgsmass_fig1_comb.jpg)\n",
    "\n",
    "Obviously this isn't exactly a Gaussian and a uniform distribution, but a line feature superimposed upon a background is the sort of thing that a physicist might see and is pretty close to this case for a local region around the feature of interest.  This is the example discussed in Ivezic $\\S$5.6.5.\n",
    "\n",
    "For this example, we will assume that the location parameter, $\\mu$, is known (say from theory) and that the errors in $x_i$ are negligible compared to $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The likelihood of obtaining a measurement, $x_i$, in this example can be written as\n",
    "$$L = p(x_i|A,\\mu,\\sigma,I) = \\frac{A}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right) + \\frac{1-A}{W}.$$\n",
    "\n",
    "Here the background probability is taken to be $0 < x < W$ and 0 otherwise.  The feature of interest lies between $0$ and $W$.  $A$ and $1-A$ are the relative strengths of the two components, which are obviously anti-correlated.  Note that there will be covariance between $A$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we adopt a uniform prior in both $A$ and $\\sigma$:\n",
    "$$p(A,\\sigma|I) = C, \\; {\\rm for} \\; 0\\le A<A_{\\rm max} \\; {\\rm and} \\; 0 \\le \\sigma \\le \\sigma_{\\rm max},$$\n",
    "then the posterior pdf is given by\n",
    "$$\\ln [p(A,\\sigma|\\{x_i\\},\\mu,W)] = \\sum_{i=1}^N \\ln \\left[\\frac{A}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\right)  + \\frac{1-A}{W} \\right].$$\n",
    "\n",
    "The figure below (Ivezic, 5.13) shows an example for $N=200, A=0.5, \\sigma=1, \\mu=5, W=10$.  Specifically, the bottom panel is a result drawn from this distribution and the top panel is the likelihood distribution derived from the data in the bottom panel.\n",
    "![Ivezic, Figure 5.13](http://www.astroml.org/_images/fig_likelihood_gausslin_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more realistic example might be one where all three parameters are unknown: the location, the width, and the background level.  But that will have to wait until $\\S$5.8.6.\n",
    "\n",
    "In the meantime, note that we have not binned the data, $\\{x_i\\}$.  We only binned Figure 5.13 for the sake of visualizaiton.  However, sometimes the data are inherently binned (e.g., the detector is pixelated).  In that case, the data would be in the form of $(x_i,y_i)$, where $y_i$ is the number of counts at each location.  We'll skip over this example, but you can read about it in Ivezic $\\S$5.6.6.  A refresher on the Poission distribution (Ivezic $\\S$3.3.4) might be appropriate first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last time we talked about not just determining the best-fit parameters for your chosen model, but also about how to decide which model is best.\n",
    "\n",
    "Let's use the example from http://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/\n",
    "to illustrate some ideas about model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell to load all of the modules we'll need and define the data array.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "\n",
    "# generate (x,y, sigma_y) \"data\" \n",
    "data = np.array([[ 0.42,  0.72,  0.  ,  0.3 ,  0.15,\n",
    "                   0.09,  0.19,  0.35,  0.4 ,  0.54,\n",
    "                   0.42,  0.69,  0.2 ,  0.88,  0.03,\n",
    "                   0.67,  0.42,  0.56,  0.14,  0.2  ],\n",
    "                 [ 0.33,  0.41, -0.25,  0.01, -0.05,\n",
    "                  -0.05, -0.12,  0.26,  0.29,  0.39, \n",
    "                   0.31,  0.42, -0.01,  0.58, -0.2 ,\n",
    "                   0.52,  0.15,  0.32, -0.13, -0.09 ],\n",
    "                 [ 0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Functions to do a polynomial fit, compute the likelihood, and determine the best-fit parameters.\n",
    "#Nothing for you to do, just run.  (But see if you can follow what is going on.)\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    # For a polynomial with order 1, this gives theta_0 + theta_1*x\n",
    "    # For a polynomial with order 2, this gives theta_0 + theta_1*x + theta_2*x^2, etc.\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, data, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    x, y, sigma_y = data\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y, y_fit, sigma_y))\n",
    "\n",
    "# a direct optimization approach is used to get best model \n",
    "# parameters (which minimize -logL)\n",
    "def best_theta(degree, model=polynomial_fit, data=data):\n",
    "    theta_0 = (degree + 1) * [0]\n",
    "    neg_logL = lambda theta: -logL(theta, data, model)\n",
    "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell.  See if you understand what it is doing.\n",
    "x, y, sigma_y = data\n",
    "Ndata = x.size\n",
    "\n",
    "# get best-fit parameters for linear, quadratic and cubic models\n",
    "theta1 = best_theta(1, data=data)\n",
    "theta2 = best_theta(2, data=data)\n",
    "theta3 = best_theta(3, data=data)\n",
    "# generate best fit lines on a fine grid \n",
    "xgrid = np.linspace(-0.1, 1.1, 1000)\n",
    "yfit1 = polynomial_fit(theta1, xgrid)\n",
    "yfit2 = polynomial_fit(theta2, xgrid)\n",
    "yfit3 = polynomial_fit(theta3, xgrid)\n",
    "\n",
    "# plot \n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
    "ax.plot(xgrid, yfit1, label='best linear model')\n",
    "ax.plot(xgrid, yfit2, label='best quadratic model')\n",
    "ax.plot(xgrid, yfit3, label='best cubic model')\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set(xlabel='x', ylabel='y', title='data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Exit Ticket Question: \n",
    "#Can you write down the equation for the cubic fit?\n",
    "#Move this cell to the top of the notebook (so that we can find it easily!)\n",
    "\n",
    "print(theta1,theta2,theta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use $\\chi^2$ per degree of freedom to determine which fit is \"best\".  It is computed as \n",
    "$$ \\chi^2_{dof}  = \\frac{1}{N-k} \\sum_i^N \\left( \\frac{y - y_{fit}}{\\sigma_y} \\right)^2, $$\n",
    "where $N$ is the number of data points and $k$ is the number of free model parameters (here 2, 3, and 4).\n",
    "\n",
    "For large values of $(N-k)$ (larger than about 10), the distribution of \n",
    "$\\chi^2$ per degre of freedom is approximately Gaussian with a width of\n",
    "$\\sigma=\\sqrt{2/(N-k)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Complete and execute this cell to compute chi2: sum{[(y-yfit)/sigma_y]^2} \n",
    "chi21 = np.sum(((y-polynomial_fit(theta1, x))/sigma_y)**2) \n",
    "chi22 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "chi23 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "# normalize by the number of degrees of freedom\n",
    "# the number of fitted parameters is 2, 3, 4\n",
    "chi2dof1 = chi21/(Ndata - 2)\n",
    "chi2dof2 = chi22/(____)\n",
    "chi2dof3 = chi23/(____)\n",
    "\n",
    "print(\"CHI2:\")\n",
    "print('   best linear model:', chi21)\n",
    "print('best quadratic model:', chi22)\n",
    "print('    best cubic model:', chi23)\n",
    "print(\"CHI2 per degree of freedom:\")\n",
    "print('   best linear model:', chi2dof1)\n",
    "print('best quadratic model:', chi2dof2)\n",
    "print('    best cubic model:', chi2dof3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously, the cubic model has the lowest $\\chi^2$ (\"fits the best\"), but \n",
    "it has 4 free parameters while the linear model has only 2 free parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How far do you have to move the last data point (currently at $x=0.88$) to the right (or down) to have the best fit be quadratic instead of linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Which model should we adopt?**\n",
    "\n",
    "We can always increase the number of paramters in our model and improve the fit.  So we need some kind of \"scoring\" system that accounts for the complexity of the model.\n",
    "\n",
    "Let's determine if we are perhaps **over** fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common scoring system is the **Aikake information criterion (AIC)**. For $N$ data points and a model with $k$ parameters, it is computed as\n",
    "$${\\rm AIC} \\equiv -2 \\ln (L_0(M)) + 2k + \\frac{2k(k+1)}{N-k-1},$$\n",
    "where the 2nd and 3rd terms are designed to penalize complex models relative to simple ones.\n",
    "\n",
    "Another scoring system is the **Bayesian information criterion (BIC)**, which is computed as\n",
    "$${\\rm BIC} \\equiv -2 \\ln (L_0(M)) + k \\ln N.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll do more with this later (and for homework), but for now I'll illustrate this with an example showing an attempt to fit a complex 1-D distribution with multiple Gaussians (from Ivezic Figure 4.2). $\\chi^2$ would keep falling with more components, but using the AIC or BIC we find that 3 Gaussians provides the best fit.\n",
    "\n",
    "![Ivezic, Figure 4.2](http://www.astroml.org/_images/fig_GMM_1D_1.png)\n",
    "\n",
    "Now let's calculate AIC and BIC for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Complete and execute this cell\n",
    "L = [] #Array with all of the likelihood values\n",
    "A = [] #Array with all of the AIC values\n",
    "B = [] #Array with all of the  BICvalues\n",
    "neg_LogL1 = -logL(theta1, data)+10  #+10 for plotting purposes\n",
    "neg_LogL2 = -logL(theta2, data)+10\n",
    "neg_LogL3 = -logL(theta3, data)+10\n",
    "L = np.append(L,neg_LogL1)\n",
    "L = np.append(L,neg_LogL2)\n",
    "L = np.append(L,neg_LogL3)\n",
    "\n",
    "xplot = np.arange(len(L))+1 #x values for plotting.\n",
    "\n",
    "AIC1 = 2.*neg_LogL1 + 2.*len(theta1) + 2.*len(theta1)*(len(theta1)+1)/(len(data[0])-len(theta1)-1)\n",
    "AIC2 = ____ #Complete\n",
    "AIC3 = ____ #Complete\n",
    "A = np.append(A,AIC1)\n",
    "A = np.append(A,AIC2)\n",
    "A = np.append(A,AIC3)\n",
    "BIC1 = 2.*neg_LogL1 + len(theta1)*np.log(len(data[0]))\n",
    "BIC2 = ____ #Complete\n",
    "BIC3 = ____ #Complete\n",
    "B = np.append(B,BIC1)\n",
    "B = np.append(B,BIC2)\n",
    "B = np.append(B,BIC3)\n",
    "\n",
    "print(\"-logL:\", L)\n",
    "print(\"AIC:\", A)\n",
    "print(\"BIC:\", B)\n",
    "\n",
    "\n",
    "fig = plt.subplots(figsize=(16, 8))\n",
    "plt.plot(xplot, L, label='Likelihood+10')\n",
    "plt.plot(xplot, A, label='AIC')\n",
    "plt.plot(xplot, B, label='BIC')\n",
    "plt.legend()\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Score\")\n",
    "print(L)\n",
    "print(B)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we interpret this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "In *hypothesis testing* we are essentially comparing a model, $M_1$, to its complement.  That is $p(M_1) + p(M_2) = 1$.  If we take $M_1$ to be the \"null\" (default) hypothesis (which is generally that, for example, a correlation does *not* exist), then we are asking whether or not the data reject the null hypothesis.\n",
    "\n",
    "In classical hypothesis testing we can ask whether or not a single model provides a good description of the data.  \n",
    "\n",
    "In Bayesian hypothesis testing, we need to have an alternative model to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "GTR: Null is a terrible word.  Boring, depressing, etc. would be better.  The null hypothesis is for example, the that cancer drug that you convinced your boss to spend millions of dollars developing doesn't actually work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Comparison\n",
    "\n",
    "In the Bayesian context, to determine which model is better we can also compute the ratio of the posterior probabilities or the **odds ratio** for two models as\n",
    "$$O_{21} \\equiv \\frac{p(M_2|D,I)}{p(M_1|D,I)}.$$\n",
    "\n",
    "Since \n",
    "$$p(M|D,I) = \\frac{p(D|M,I)p(M|I)}{p(D|I)},$$\n",
    "the odds ratio can ignore $p(D|I)$ since it  will be the same for both models.  \n",
    "\n",
    "(That is even more important than you might think as the denominator is the integral of the numerator, but what if you don't have an analytical function that you can integrate?!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we interpret the values of the odds ratio in practice?\n",
    "\n",
    "Jeffreys proposed a five-step scale for interpreting the odds ratio, where $O_{21} > 10$ represents “strong” evidence in favor of $M_2$ ($M_2$ is ten times more probable than $M_1$), and $O_{21} > 100$ is “decisive” evidence ($M_2$ is one hundred times more probable than $M_1$). When $O_{21} < 3$, the evidence is “not worth more than a bare mention.”\n",
    "\n",
    "But note:\n",
    "  * These are just **definitions of conventions**, i.e., a way to give a quantitative meaning to qualitative phrases.\n",
    "  * The odds ratio **compares** the models, it doesn't tell us about the *absolute* goodness of fit: model A can be 100x better than model B, but still be pretty lousy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin Flip as an Example of Bayesian Model Comparison\n",
    "\n",
    "Let's look at an example using coin flips. Let's assume we have $N$ draws and $k$ are \"success\" (say, heads). \n",
    "\n",
    "We will compare two hypotheses:\n",
    "\n",
    "**M1**: the coin has a known heads probability $b_\\ast$ (say, a fair coin with $b_\\ast=0.5$), with a prior given by a delta function, $\\delta(b-b_\\ast)$,\n",
    "\n",
    "and\n",
    "\n",
    "**M2**: the heads probability $b$ is unknown, with a uniform prior in the range 0–1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "GTR: Basically M1 is an fair coin.  M2 is an unfair coin.  But the way that we do it, we are testing all possible values of the heads probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model that we need (parametrized by the probability of success $b$, with $k$ successes) is the binomial distribution (which we haven't talked about): \n",
    "  $$    p(k\\,|\\,b, N) = \\frac{N!}{k! \\, (N-k)!} \\, b^k \\, (1-b)^{N-k} $$\n",
    "\n",
    "For model M2 the prior for $b$ is flat in the range 0-1 and the product of the \n",
    "data likelihood and prior is same as above. However, for model M1 the prior is a \n",
    "delta function $\\delta(b-b_\\ast)$ and we get for the product of the \n",
    "data likelihood and prior  \n",
    "$$    p(k\\,|\\,b_\\ast, N, M1)\\,p(b|M1, I) = \\frac{N!}{k! \\, (N-k)!} \\, b_\\ast^k \\, (1-b_\\ast)^{N-k}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consequently, the odds ratio is given by \n",
    "$$ O_{21} = \\int_0^1 \\left(\\frac{b}{b_\\ast}\\right)^k \\left(\\frac{1-b}{1-b_\\ast}\\right)^{N-k} db, $$\n",
    "as illustrated in the following figure. \n",
    "\n",
    " \n",
    "![Ivezic, Figure 5.1](http://www.astroml.org/_images/fig_odds_ratio_coin_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This figure (from the textbook) illustrates the behavior of $O_{21}$ as a function of $k$\n",
    "for two different values of N and for two different values of $b_\\ast$: $b_\\ast = 0.5$ \n",
    "(M1: the coin is fair) and $b_\\ast = 0.1$ (M1: the coin is biased to tails). As the figure shows, the ability to distinguish \n",
    "the two hypothesis **improves** with the sample size. \n",
    "\n",
    "For example, when $b_\\ast= 0.5$ and \n",
    "$k/N = 0.1$ ($k=1$ on the left and $k=2$ on the right), the odds ratio in favor of M2 (that the coin may not be fair) increases from about 9 for $N=10$ to about \n",
    "263 for $N=20$. When k = $b_\\ast N$, the odds ratio is 0.37 for $N=10$ and 0.27 for $N=20$. \n",
    "In other words, **the simpler model is favored by the data**, and the support strengthens \n",
    "with the sample size. \n",
    "\n",
    "Integration of the above equation reveals that \n",
    "$O_{21}= \\sqrt{\\pi/(2N)}$ when k = $b_\\ast N$ and $b_\\ast = 0.5$. That means that, to build strong \n",
    "evidence that a coin is fair, $O_{21} < 0.1$, it takes as many as N $>$ 157 tosses. With \n",
    "N = 10,000, the heads probability of a fair coin is measured with a precision of 1% and\n",
    "the corresponding odds ratio is $O_{21} \\approx 1/80$, approaching Jeffreys’ decisive \n",
    "evidence level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GTR: Show this?  Have them show this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In **frequentist approach**, we would ask whether we can reject the \"null hypothesis\" that \n",
    "our coin is fair. Specifically, we would ask whether a given $k$ is a very unusual outcome (at some \n",
    "significance level $\\alpha$, say $\\alpha=0.05$, which corresponds to about \"2$\\sigma$\"\n",
    "deviation) for a fair coin with $b_\\ast = 0.5$ and with\n",
    "a given N. In the **Bayesian approach**, we offer an alternative hypothesis that the coin \n",
    "has an unknown heads probability. While this probability can be estimated from provided \n",
    "data ($b_0$), **we consider all the possible values** of $b_0$ when comparing the two proposed \n",
    "hypotheses. \n",
    "\n",
    "As a numerical example, let's consider N=20 and k=16. The scatter around the expected value $k_0 = b_\\ast N$ = 10 is $\\sigma_k = 2.24$. \n",
    "Therefore, k = 16 is about 2.7$\\sigma_k$ away from $k_0$, and at the adopted significance \n",
    "level $\\alpha=0.05$ we **reject the null hypothesis** (that our coin was fair).  This rejection means that it is unlikely that k = 16 would have arisen by chance). Of course, k = 16 does **not** imply \n",
    "that it is impossible that the coin is fair (infrequent events happen, too!).\n",
    "\n",
    "As shown in the above figure, the chosen parameters (N=20 and k=16) correspond to the \n",
    "Bayesian **odds ratio** of about 10 in favor of hypothesis M2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want to learn about or get a refresher on classical inference in detail, you might go through the full set of videos on it at \n",
    "[Khan Academy](https://www.khanacademy.org/math/ap-statistics/tests-significance-ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov-Chain Monte Carlo Methods\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Consider the problem of estimating location and scale parameters\n",
    "for a sample drawn from a Gaussian distribution that we introduced earlier.\n",
    "We had a two-dimensional posterior pdf for $\\mu$ and $\\sigma$:\n",
    "\n",
    "![Ivezic, Figure 5.5](http://www.astroml.org/_images/fig_likelihood_gaussian_1.png)\n",
    "\n",
    "\n",
    "It was easy to numerically integrate the posterior pdf, as well \n",
    "as to find its maximum, using brute force grid search because\n",
    "it was only a two-dimesional problem. With 100 grid points per\n",
    "coordinate it was only $10^4$ values. However, even in a case\n",
    "of rather simple 5-dimensional problem (as we'll discuss later \n",
    "today), we'd have $10^{10}$ values! And often we work with models \n",
    "of much higer dimensionality (it can be thousands). \n",
    "\n",
    "You could simply randomly sample the grid at every point, and try to find the minimum based on that. But that can also be quite time consuming, and you will spend a lot of time in regions of parameter space that yields small likelihood.\n",
    "\n",
    "A better way is to adopt a Markov-Chain Monte Carlo (MCMC). MCMC gives us a way to make this problem computationally tractable by sampling the full multi-dimensional parameter space, in a way that builds up the most density in the regions of parameter space which are closest to the maximum. Then, you can post-process the “chain” to infer the distribution and error regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 5.22 shows a problem similar to the one above, done with a Markov Chain Monte Carlo.  The dashed lines are the known (analytic) solution.  The solid lines are from the MCMC estimate with 10,000 sample points.\n",
    "![Ivezic, Figure 5.10](http://www.astroml.org/_images/fig_cauchy_mcmc_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does MCMC work?\n",
    "\n",
    "I've really struggled to come up with a simple way of illustrating MCMC so that you (and I for that matter) can understand it.  Unfortunately, even the supposedly dumbed-down explanations are really technical.   But let's see what I can do!  \n",
    "\n",
    "Let's start by simply trying to understand what a Monte Carlo is and what a Markov Chain is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Monte Carlo?\n",
    "\n",
    "In case you are not familiar with Monte Carlo methods, it might help to know that the term is derived from the name of an administrative area of the Principality of Monaco (Monte Carlo) where they are known for gambling.  And gambling and random sampling go together.\n",
    "\n",
    "We'll consider a simple example: you have forgotten the value of $\\pi$, but you know the formula for the area of a square and how to draw a circle. \n",
    "\n",
    "We can use the information that we *do* know to numerically compute $\\pi$.\n",
    "\n",
    "We start by drawing a square and circumscribing a circle in it (actually it suffices to just do a quarter of a circle and scale accordingly).  Then we put down random points within the square and note which ones land in the circle.  The ratio of random points in the circle to the number of random points drawn is related to the area of our circle, allowing us to calculate $\\pi$.  Using more random points yields more precise estimates of the area.\n",
    "\n",
    "Try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll consider one quadrant of a square of sides [-1,1] in which we inscribe a circle (actually just one quarter of a circle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "fig, ax = plt.subplots(subplot_kw=dict(aspect='equal'))\n",
    "#ax.axis([0, 1, 0, 1], aspect='equal');\n",
    "ax.axis([0, 1, 0, 1]);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lay down M random points.  Tally how many are within a unit circle.\n",
    "M = 1000\n",
    "x = stats.uniform(___,___).rvs(___) #M random draws between 0 and 1\n",
    "y = stats.uniform(___,___).rvs(___) #M random draws between 0 and 1\n",
    "r2 = ____ #Radius of cirle\n",
    "fig, ax = plt.subplots(subplot_kw=dict(aspect='equal'))\n",
    "plt.plot(x, y, '.k', markersize=3, c='blue') #Plot points in square\n",
    "plt.plot(x[r2<1], y[r2<1], '.k', markersize=3, c='red') #Plot points also in circle\n",
    "ax.axis([0, 1, 0, 1], aspect='equal');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "    \n",
    "#Asquare = d^2, Acircle = pi*d^2\n",
    "\n",
    "piEst = 4.0*np.sum(r2<1)/M # pi = 4A/d^2, where A is d^2 times the ratio of points \"in\" to total points\n",
    "\n",
    "print(\"Estimate of pi is {0} for {1} draws with fractional error {2}.\".format(piEst,M,np.abs((np.pi-piEst)/np.pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See how many draws does it take to get the error down to 1 part in a thousand?\n",
    "\n",
    "See also [this Khan Academy simulation](https://www.khanacademy.org/computer-programming/monte-carlo-finding-the-value-of-pi/6530004791197696/embedded?embed=yes&article=yes&editor=no&buttons=no&author=no&autoStart=yes&width=610&height=420)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, Monte Carlo methods are about using random sampling to obtain a numerical result (e.g., the value of an integral), where there is no analytic result.\n",
    "\n",
    "In the case of the circle above, we have computed the intergral:\n",
    "$$\\int\\int_{x^2+y^2\\le 1} dx dy.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Markov Chain?\n",
    "\n",
    "A Markov Chain is defined as a sequence of random variables where a parameter depends *only* on the preceding value.  Such processes are \"memoryless\".  \n",
    " \n",
    "Mathematically, we have\n",
    "$$p(\\theta_{i+1}|\\{\\theta_i\\}) = p(\\theta_{i+1}|\\theta_i).$$\n",
    "\n",
    "Now, if you are like me, you might look at that and say \"Well, day 3 is based on day 2, which is based on day 1, so day 3 is based on day 1...\".\n",
    "\n",
    "So let's look at an example to see what we mean and how this might be a memoryless process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say that you are an astronomer and you want to know how likely it is that tomorrow night will be clear given the weather tonight (clear or cloudy).  From past history, you know that:\n",
    "\n",
    "$$p({\\rm clear \\; tomorrow} \\, |\\,  {\\rm cloudy \\; today}) = 0.5,$$\n",
    "which means that\n",
    "$$p({\\rm cloudy \\; tomorrow} \\, |\\, {\\rm cloudy \\; today}) = 0.5.$$\n",
    "\n",
    "We also have\n",
    "$$p({\\rm cloudy \\; tomorrow} \\, |\\, {\\rm clear \\; today}) = 0.1,$$\n",
    "which means that\n",
    "$$p({\\rm clear \\; tomorrow} \\, |\\, {\\rm clear \\; today}) = 0.9.$$\n",
    "\n",
    "(That is, you don't live in Philadelphia.)\n",
    "\n",
    "We can start with the sky conditions today and make predictions going forward.  This will look like a big decision tree.  After enough days, we'll reach equilibrium probabilities that have to do with the mean weather statistics (ignoring seasons) and we'll arrive at\n",
    "\n",
    "$$p({\\rm clear}) = 0.83,$$\n",
    "and \n",
    "$$p({\\rm cloudy}) = 0.17.$$\n",
    "\n",
    "You get the same answer for day $N$ as day $N+1$ and it doesn't matter whether is was clear to cloudy on the day that you started.    \n",
    "\n",
    "The steps that we have taken in this process are a **Markov Chain**."
   ]
  },
  {
   "attachments": {
    "clearcloudy.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are illustration of the process from an article in [towarddatascience.com](https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d).\n",
    "\n",
    "![clearcloudy.png](attachment:clearcloudy.png)"
   ]
  },
  {
   "attachments": {
    "Chain.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sampling for 10,000 days using this prescription gives a chain that looks like this:\n",
    "\n",
    "![Chain.png](attachment:Chain.png)\n",
    "For homework, you will generate an equivalent plot, testing to see whether or not it matters if you start on a clear day or a cloudy day."
   ]
  },
  {
   "attachments": {
    "ChainHist.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A histogram of the above plot reveals the distribution of p(clear) values over this time.  We can use this to determine the most likely value and an error on our estimate.\n",
    "\n",
    "![ChainHist.png](attachment:ChainHist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In MCMC the process must be **stationary** which basically means that its looks the same no matter where you sample it.  \n",
    "\n",
    "Obviously that isn't going to be the case in the early steps of the chain.  In our example above, after some time the process was stationary, but not in the first few days.\n",
    "\n",
    "So, there is a **burn-in** phase that needs to be discarded.  How one determines how long many iterations the burn-in should last when you don't know the distribution can be a bit tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo Summary\n",
    "\n",
    "1. Starting at a random position, evaluate the likelihood.\n",
    "2. Choose a new position, according to some transition probabilities, and evaluate the likelihood there.\n",
    "3. Examine the odds ratio formed by the new-position likelihood and the old-position likelihood. If the odds ratio is greater than 1, move to the new position.  If it is less than one, keep it under the following conditions: draw a random number between zero and 1.   If the odds ratio is larger than the random number, keep it. If not, reject the new position.\n",
    "4. Repeat 1-3 many times. After a period of time (the burn-in) the simulation should reach an equilibrium. Keep the results of the chain (after burn-in), and postprocess those results to infer the likelihood surface.\n",
    "\n",
    "Step 3 is crucial.  In order to fully explore the space, sometimes you have to go in what seems to be the wrong direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most of the difficulty in the MCMC process comes from either determining the burn-in or deciding how to step from one position to another.  In our circle example we have drawn points in a completely random manner.  However, that may not be the most efficient manner to span the space.  \n",
    "\n",
    "The most commonly used algorithm for stepping from one position to another is the [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) algorithm.  \n",
    "\n",
    "In astronomy, the ${\\tt emcee}$ algorithm has become more popular in recent years.  We won't discuss either in detail, but both the [code](http://dan.iel.fm/emcee/current/) and a [paper](http://adsabs.harvard.edu/abs/2013PASP..125..306F) describing the ${\\tt emcee}$ are available.\n",
    "\n",
    "Recall that our parameter space is multidimensional.  So, when you are stepping from one point to another, you are really doing it in N-D parameter space!  You might wonder if you could just step one parameter at a time.  Sure!  That's what [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) does (and it is less likely to get stuck in a local minimum than Metropolis-Hastings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then end result of this process will be a chain of likelihoods that we can use to compute the likelihood contours.  \n",
    "\n",
    "If you are using MCMC, then you probably have multiple parameters (otherwise, you'd be doing something easier).  So, it helps to display the parameters two at a time, marginalizing over the other parameters.  An example is given in Ivezic, Figure 5.24, which compares the model results for a single Gaussian fit to a double Gaussian fit:\n",
    "\n",
    "![Ivezic, Figure 5.24](http://www.astroml.org/_images/fig_model_comparison_mcmc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll end by going through the example given at\n",
    "[http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/).\n",
    "\n",
    "First set up some stuff by executing the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's generate some data points and plot them.  We'll try a normal distribution, centered at 0 with 100 data points, using [numpy.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html).  Our goal is to estimate $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.random.randn(100)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(data)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have to pick a model to try.  For the sake of simplicity for this example, we'll assume a normal distribution: $\\mathscr{N}(\\mu,\\sigma=1)$ (i.e., with $\\sigma=1$).  We'll also assume a normal distribution for the prior on $\\mu$: $\\mathscr{N}(0,1)$.\n",
    "\n",
    "We can use that to write a function for our posterior distribution as follows (recalling from Lecture 3 the formula for the product of two Gaussians):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell\n",
    "def calc_posterior_analytical(data, xgrid, mu_0, sigma_0):\n",
    "    sigma = 1.\n",
    "    n = len(data)\n",
    "    #Error-weighted sum of prior and average of data\n",
    "    #Recall from lecture 3 the formula for the product of 2 Gaussians\n",
    "    mu_post = (mu_0 / sigma_0**2 + data.sum() / sigma**2) / (1. / sigma_0**2 + n / sigma**2)\n",
    "    # This is the Gaussian for the prior: mu_0 / sigma_0**2\n",
    "    # This is the Gaussian for the likelihood: data.sum() / sigma**2\n",
    "    # We are deriving the mean from the sum of the data values\n",
    "    var_post = (1. / sigma_0**2 + n / sigma**2)**-1\n",
    "    return norm.pdf(xgrid, mu_post, np.sqrt(var_post))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "xgrid = np.linspace(-1, 1, 500)\n",
    "posterior_analytical = calc_posterior_analytical(data, xgrid, 0., 1.)\n",
    "plt.plot(xgrid, posterior_analytical)\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('post prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we need to sample the distribution space.  Let's start by trying $\\mu_0 = -1$ and evaluate.\n",
    "\n",
    "Then we'll jump to a new position using one of the algorithms mentioned above.  In this case we'll use the Metropolis algorithm, which draws the new points from a normal distribution centered on the current guess for $\\mu$.\n",
    "\n",
    "Next we evaluate whether that jump was \"good\" or not -- by seeing if the value of likelihood\\*prior increases.  Now, we want to get the right answer, but we also want to make sure that we sample the full parameter space (so that we don't get stuck in a local minimum).  So, even if the this location is not better than the last one, we'll have some probability of staying there anyway.  \n",
    "\n",
    "The reason that taking the ratio of likelihood\\*prior works is that the denominator drops out.  That's good because the denominator is the integral of the numerator and that's what we are trying to figure out!  In short, we don't have to know the posterior probability to know that the posterior probability at one step is better than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# See https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/MCMC-sampling-for-dummies.ipynb\n",
    "def sampler(data, samples=4, mu_init=.5, proposal_width=0.5, plot=False, mu_prior_mu=0, mu_prior_sd=1.):\n",
    "    mu_current = mu_init\n",
    "    posterior = [mu_current]\n",
    "    for i in range(samples):\n",
    "        # suggest new position\n",
    "        # changing the width of this distribution changes how big the jump is between trials.\n",
    "        mu_proposal = norm(mu_current, proposal_width).rvs()\n",
    "\n",
    "        # Compute likelihood by multiplying probabilities of each data point\n",
    "        likelihood_current = norm(mu_current, 1).pdf(data).prod()\n",
    "        likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n",
    "        \n",
    "        # Compute prior probability of current and proposed mu        \n",
    "        prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n",
    "        prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n",
    "        \n",
    "        p_current = likelihood_current * prior_current\n",
    "        p_proposal = likelihood_proposal * prior_proposal\n",
    "        \n",
    "        # Accept proposal?\n",
    "        p_accept = p_proposal / p_current\n",
    "        \n",
    "        # Usually would include prior probability, which we neglect here for simplicity\n",
    "        # Accept if p_accept>1 (since random limited to [0,1]) and if p_accept > rand as well\n",
    "        accept = np.random.rand() < p_accept\n",
    "        \n",
    "        if plot:\n",
    "            plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accept, posterior, i)\n",
    "        \n",
    "        if accept:\n",
    "            # Update position\n",
    "            mu_current = mu_proposal\n",
    "        \n",
    "        posterior.append(mu_current)\n",
    "        \n",
    "    return posterior\n",
    "\n",
    "# Function to display\n",
    "def plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accepted, trace, i):\n",
    "    from copy import copy\n",
    "    trace = copy(trace)\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "    fig.suptitle('Iteration %i' % (i + 1))\n",
    "    xgrid = np.linspace(-3, 3, 5000)\n",
    "    color = 'g' if accepted else 'r'\n",
    "        \n",
    "    # Plot prior\n",
    "    prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n",
    "    prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n",
    "    prior = norm.pdf(xgrid,mu_prior_mu, mu_prior_sd)\n",
    "    ax1.plot(xgrid, prior)\n",
    "    ax1.plot([mu_current] * 2, [0, prior_current], marker='o', color='b')\n",
    "    ax1.plot([mu_proposal] * 2, [0, prior_proposal], marker='o', color=color)\n",
    "    ax1.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax1.set(ylabel='Probability Density', title='current: prior(mu=%.2f) = %.2f\\nproposal: prior(mu=%.2f) = %.2f' % (mu_current, prior_current, mu_proposal, prior_proposal))\n",
    "    \n",
    "    # Likelihood\n",
    "    likelihood_current = norm(mu_current, 1).pdf(data).prod()\n",
    "    likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n",
    "    y = norm.pdf(xgrid,loc=mu_proposal, scale=1)\n",
    "    #sns.distplot(data, kde=False, norm_hist=True, ax=ax2)\n",
    "    ax2.hist(data,alpha=0.5,density='True')\n",
    "    ax2.plot(xgrid, y, color=color)\n",
    "    ax2.axvline(mu_current, color='b', linestyle='--', label='mu_current')\n",
    "    ax2.axvline(mu_proposal, color=color, linestyle='--', label='mu_proposal')\n",
    "    #ax2.title('Proposal {}'.format('accepted' if accepted else 'rejected'))\n",
    "    ax2.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax2.set(title='likelihood(mu=%.2f) = %.2f\\nlikelihood(mu=%.2f) = %.2f' % (mu_current, 1e14*likelihood_current, mu_proposal, 1e14*likelihood_proposal))\n",
    "    \n",
    "    # Posterior\n",
    "    posterior_analytical = calc_posterior_analytical(data, xgrid, mu_prior_mu, mu_prior_sd)\n",
    "    ax3.plot(xgrid, posterior_analytical)\n",
    "    posterior_current = calc_posterior_analytical(data, mu_current, mu_prior_mu, mu_prior_sd)\n",
    "    posterior_proposal = calc_posterior_analytical(data, mu_proposal, mu_prior_mu, mu_prior_sd)\n",
    "    ax3.plot([mu_current] * 2, [0, posterior_current], marker='o', color='b')\n",
    "    ax3.plot([mu_proposal] * 2, [0, posterior_proposal], marker='o', color=color)\n",
    "    ax3.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    #x3.set(title=r'prior x likelihood $\\propto$ posterior')\n",
    "    ax3.set(title='posterior(mu=%.2f) = %.5f\\nposterior(mu=%.2f) = %.5f' % (mu_current, posterior_current, mu_proposal, posterior_proposal))\n",
    "    \n",
    "    if accepted:\n",
    "        trace.append(mu_proposal)\n",
    "    else:\n",
    "        trace.append(mu_current)\n",
    "    ax4.plot(trace)\n",
    "    ax4.set(xlabel='iteration', ylabel='mu', title='trace')\n",
    "    plt.tight_layout()\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualize the sampling, we'll create plots for some quantities that are computed. Each row below is a single iteration through our Metropolis sampler.\n",
    "\n",
    "The first column is the prior distribution -- our belief about $\\mu$ before seeing the data.  The distribution is static and we are only plugging in our $\\mu$ proposals. The vertical lines represent the current $\\mu$ in blue and the proposed $\\mu$ in either red or green (rejected or accepted, respectively).\n",
    "\n",
    "The 2nd column is the likelihood -- what we are using to evaluate how good our model explains the data.  The likelihood function changes in response to the proposed $\\mu$. The blue histogram is the data. The solid line in green or red is the likelihood with the currently proposed mu. The more overlap there is between likelihood and data, the better the model explains the data and the higher the resulting probability will be. The dashed line of the same color is the proposed mu and the dashed blue line is the current mu.\n",
    "\n",
    "The 3rd column is the (normalized) posterior distribution.\n",
    "\n",
    "The 4th column is the trace (i.e. the posterior samples of $\\mu$ we are generating) where we store each sample irrespective of whether it was accepted or rejected (in which case the line just stays constant).\n",
    "\n",
    "Note that we always move to relatively more likely $\\mu$ values (in terms of their posterior density), but only sometimes to relatively less likely $\\mu$ values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sampler(data, samples=8, mu_init=-1., plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens when we do this lots of times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "posterior = sampler(data, samples=15000, mu_init=-1.)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(posterior)\n",
    "_ = ax.set(xlabel='sample', ylabel='mu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Making a histogram of these results is our estimated posterior probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "ax.hist(posterior[500:],bins=30,alpha=0.5,density='True',label='estimated posterior')\n",
    "xplot = np.linspace(-.5, .5, 500)\n",
    "post = calc_posterior_analytical(data, xplot, 0, 1)\n",
    "ax.plot(xplot, post, 'g', label='analytic posterior')\n",
    "_ = ax.set(xlabel='mu', ylabel='belief');\n",
    "ax.legend(fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our algorithm for deciding where to move to next used a normal distribution where the mean was the current value and we had to assume a width.  Find where we specified that and see what happens if you make it a lot smaller or a lot bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Complex Models\n",
    "\n",
    "The example above was overkill in that we were only trying to estmate $\\mu$.  Note also that we can do this in less than 10 lines using the ${\\tt pymc3}$ module.\n",
    "\n",
    "The process is essentially the same when you add more parameters.  Check out this [animation of a 2-D process](http://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/) by the same author whose example we just followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Markov Chain Monte Carlo in Practice\n",
    "\n",
    "For more involved examples, we will use two popular python modules:\n",
    "PyMC and emcee. Quoting \n",
    "[Jake's blog:](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)\n",
    "\n",
    "** emcee **\n",
    "\n",
    "*The emcee package (also known as MCMC Hammer, which is in the running for best Python package name in history) is a Pure Python package written by Astronomer Dan Foreman-Mackey. It is a lightweight package which implements a fairly sophisticated Affine-invariant Hamiltonian MCMC. Because the package is pure Python (i.e. it contains no compiled extensions) it is extremely easy to install; with pip, simply type at the command-line \"pip install emcee\".*\n",
    "\n",
    "*Emcee does not have much specific boilerplate code; it simply requires you to pass it a Python function which returns a value proportional to the log-posterior probability, and returns samples from that posterior.*\n",
    "\n",
    "** PyMC **\n",
    "\n",
    "*The PyMC package has many more features than emcee, including built-in support for efficient sampling of common prior distributions. PyMC by default uses the classic Metropolis-Hastings sampler, one of the earliest MCMC algorithms. For performance, it uses compiled fortran libraries, so it is less trivial to install using tools like pip. PyMC binaries for many systems can be quite easily installed with conda.*\n",
    "\n",
    "\n",
    "More details about PyMC are available from [the pyMC User Guide](https://pymc-devs.github.io/pymc/), but note that we are going to be using [PyMC3](https://docs.pymc.io/).\n",
    "\n",
    "\n",
    "We will first do an example using emcee and then we will do an example with pyMC.  We won't go over these in class, rather they are provided here for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "from astroML.plotting import hist\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Markov Chain Monte Carlo with emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "np.random.seed(21)\n",
    "Ndata = 100\n",
    "mu = 1.0\n",
    "sigma = 0.5 # assumed known \n",
    "data = stats.norm(mu, sigma).rvs(Ndata)\n",
    "\n",
    "def Likelihood(x,sigma, data):\n",
    "    # Gaussian likelihood \n",
    "    return np.prod(np.exp(-(data-x)**2/2/sigma**2))\n",
    "\n",
    "def Prior(x):\n",
    "    return 1.0/10   # flat: it cancels out and has no effect \n",
    "\n",
    "def myPosterior(x, sigma, data):\n",
    "    return Likelihood(x, sigma, data)*Prior(x)\n",
    "\n",
    "# emcee wants ln of posterior pdf\n",
    "def myLogPosterior(x, sigma, data):\n",
    "    return np.log(myPosterior(x, sigma, data))\n",
    "\n",
    "# emcee combines multiple \"walkers\", each of which is its own MCMC chain. \n",
    "# the number of trace results will be nwalkers * nsteps\n",
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 6  # number of MCMC walkers\n",
    "Nburn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 5000  # number of MCMC steps to take\n",
    "\n",
    "# initialize theta \n",
    "np.random.seed(0)\n",
    "starting_guesses = np.random.random((nwalkers, ndim))\n",
    "\n",
    "# the function call where all the work happens: \n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, myLogPosterior, args=[sigma, data])\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    " \n",
    "# sampler.chain is of shape (nwalkers, nsteps, ndim)\n",
    "# throw-out the burn-in points and reshape:\n",
    "emcee_trace  = sampler.chain[:, Nburn:, :].reshape(-1, ndim).T\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot \n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, wspace=0.35, bottom=0.18)\n",
    "\n",
    "chainE = emcee_trace[0]\n",
    "M = np.size(chainE)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "xgrid = np.linspace(1,M,M)\n",
    "plt.plot(xgrid, chainE)\n",
    "ax1.axis([0, M, np.min(chainE), 1.1*np.max(chainE)])\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('chain')\n",
    "# plot running mean: \n",
    "meanC = [np.mean(chainE[:int(N)]) for N in xgrid]\n",
    "ax1.plot(xgrid, meanC,c='red',label='chain mean') \n",
    "ax1.plot(xgrid, 0*xgrid+np.mean(data),c='yellow',label='data mean')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "# skip first Nburn samples\n",
    "Nburn = 1000\n",
    "Nchain = np.size(chainE[xgrid>Nburn])\n",
    "Nhist, bins, patches = plt.hist(chainE[xgrid>Nburn], bins = 'auto', histtype = 'stepfilled')\n",
    "# plot expectations based on CLT\n",
    "binwidth = bins[1]-bins[0]\n",
    "muCLT = np.mean(data)\n",
    "sigCLT = np.std(data)/np.sqrt(Ndata)\n",
    "muGrid = np.linspace(0.7, 1.3, 500)\n",
    "gauss = Nchain*binwidth*stats.norm(muCLT, sigCLT).pdf(muGrid) \n",
    "ax2.plot(muGrid, gauss, c='red') \n",
    "ax2.set_ylabel('p(chain)')\n",
    "ax2.set_xlabel('chain values')\n",
    "ax2.set_xlim(0.7, 1.3)\n",
    "ax2.set_ylim(0, 1.2*np.max(gauss))\n",
    "ax2.set_title(r'Chain from emcee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Markov Chain Monte Carlo with PyMC\n",
    "\n",
    "Now we will use pyMC3 to get a 2-dimensional posterior pdf \n",
    "for location and scale parameters using a sample drawn from \n",
    "Cauchy distribution, similar to the distribution we considered earlier using \n",
    "brute force grid search method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 5.22\n",
    "# Author: Jake VanderPlas (adapted to PyMC3 by Brigitta Sipocz)\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import cauchy\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "\n",
    "def cauchy_logL(xi, sigma, mu):\n",
    "    \"\"\"Equation 5.74: cauchy likelihood\"\"\"\n",
    "    xi = np.asarray(xi)\n",
    "    n = xi.size\n",
    "    shape = np.broadcast(sigma, mu).shape\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
    "\n",
    "    return ((n - 1) * np.log(sigma)\n",
    "            - np.sum(np.log(sigma ** 2 + (xi - mu) ** 2), 0))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Draw the sample from a Cauchy distribution\n",
    "np.random.seed(44)\n",
    "mu_0 = 0\n",
    "gamma_0 = 2\n",
    "xi = cauchy(mu_0, gamma_0).rvs(10)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Set up and run MCMC:\n",
    "with pm.Model():\n",
    "    mu = pm.Uniform('mu', -5, 5)\n",
    "    log_gamma = pm.Uniform('log_gamma', -10, 10)\n",
    "\n",
    "    # set up our observed variable x\n",
    "    x = pm.Cauchy('x', mu, np.exp(log_gamma), observed=xi)\n",
    "\n",
    "    trace = pm.sample(draws=12000, tune=1000, cores=1)\n",
    "\n",
    "# compute histogram of results to plot below\n",
    "L_MCMC, mu_bins, gamma_bins = np.histogram2d(trace['mu'],\n",
    "                                             np.exp(trace['log_gamma']),\n",
    "                                             bins=(np.linspace(-5, 5, 41),\n",
    "                                                   np.linspace(0, 5, 41)))\n",
    "L_MCMC[L_MCMC == 0] = 1E-16  # prevents zero-division errors\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Compute likelihood analytically for comparison\n",
    "mu = np.linspace(-5, 5, 70)\n",
    "gamma = np.linspace(0.1, 5, 70)\n",
    "logL = cauchy_logL(xi, gamma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "p_mu = np.exp(logL).sum(0)\n",
    "p_mu /= p_mu.sum() * (mu[1] - mu[0])\n",
    "\n",
    "p_gamma = np.exp(logL).sum(1)\n",
    "p_gamma /= p_gamma.sum() * (gamma[1] - gamma[0])\n",
    "\n",
    "hist_mu, bins_mu = np.histogram(trace['mu'], bins=mu_bins, density=True)\n",
    "hist_gamma, bins_gamma = np.histogram(np.exp(trace['log_gamma']),\n",
    "                                      bins=gamma_bins, density=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# first axis: likelihood contours\n",
    "ax1 = fig.add_axes((0.4, 0.4, 0.55, 0.55))\n",
    "ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax1.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax1.contour(mu, gamma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='b', linestyles='dashed')\n",
    "\n",
    "ax1.contour(0.5 * (mu_bins[:-1] + mu_bins[1:]),\n",
    "            0.5 * (gamma_bins[:-1] + gamma_bins[1:]),\n",
    "            convert_to_stdev(np.log(L_MCMC.T)),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.1, 0.4, 0.29, 0.55))\n",
    "ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.plot(hist_gamma, 0.5 * (bins_gamma[1:] + bins_gamma[:-1]\n",
    "                            - bins_gamma[1] + bins_gamma[0]),\n",
    "         '-k', drawstyle='steps')\n",
    "ax2.plot(p_gamma, gamma, '--b')\n",
    "ax2.set_ylabel(r'$\\gamma$')\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# third axis: marginalized over gamma\n",
    "ax3 = fig.add_axes((0.4, 0.1, 0.55, 0.29))\n",
    "ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax3.plot(0.5 * (bins_mu[1:] + bins_mu[:-1]), hist_mu,\n",
    "         '-k', drawstyle='steps-mid')\n",
    "ax3.plot(mu, p_mu, '--b')\n",
    "ax3.set_xlabel(r'$\\mu$')\n",
    "plt.xlim(-5, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exit Ticket:  Fill in this notebook, including writing the equation for the cubic in the polynomial fitting exmaple."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
