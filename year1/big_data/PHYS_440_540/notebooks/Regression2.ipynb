{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization and Nonlinear Regression\n",
    "G. Richards\n",
    "(2016,2018,2020)\n",
    "based on materials from Connolly (especially) and Ivezic Chapter 8.3, 8.5-8.7, 8.9, 8.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "We have to be a little careful when doing regression because if we progressively increase the complexity of the model, then we reach a regime where we are overfitting the data (i.e. there are too many degrees of freedom in the model).\n",
    "\n",
    "For example, let's look at an example using Polynomial Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "orders=[3,4,5]\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" function to approximate by polynomial interpolation\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 8, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(0, 8, 10)\n",
    "#rng = np.random.RandomState(0)\n",
    "#rng.shuffle(x)\n",
    "#x = np.sort(x[:10])\n",
    "y = f(x)+0.25*(np.random.random(len(x))-0.5)\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "X = x[:, None]\n",
    "X_plot = x_plot[:, None]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "lw = 2\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw, label=\"ground truth\")\n",
    "plt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate(orders):\n",
    "    poly = PolynomialRegression(degree)\n",
    "    poly.fit(X,y)\n",
    "    y_plot = poly.predict(X_plot)\n",
    "\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is fit with order = 3, 4, and 5.  What happens if you make the order $\\sim N_{\\rm points}$?  Try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In that case we are clearly overfitting the data.  For cases where we are concerned with overfitting, instead of computing \n",
    "\n",
    "$$(Y - M \\theta)^T(Y- M \\theta),$$\n",
    "\n",
    "we can apply constraints (usually of smoothness, number of coefficients, size of coefficients):\n",
    "\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda |\\theta^T \\theta|^2,$$\n",
    "\n",
    "with $\\lambda$ as the \"regularization parameter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This leads to a solution for the parameters of the model\n",
    "\n",
    "$$\\theta = (M^T C^{-1} M + \\lambda I)^{-1} (M^T C^{-1} Y)$$\n",
    "\n",
    "with $I$ the identity matrix.\n",
    "\n",
    "From the Bayesian perspective this is the same as applying a prior like:\n",
    "\n",
    "$$p(\\theta | I ) \\propto \\exp{\\left(\\frac{-(\\lambda \\theta^T \\theta)^2}{2}\\right)}$$\n",
    "\n",
    "which, when multiplied by the likelihood for regression, gives the same answer for the maximum likelihood of the posterior probability as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression \n",
    "\n",
    "The case for such a Gaussian prior corresponds to [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), which\n",
    "penalizes the regression coefficients according to\n",
    "\n",
    "$$ |\\theta |^2 < s.$$\n",
    "\n",
    "That is the square of each parameter in the fitting is restricted to be less than some value, $s$.  We'll come back to exactly what $s$ is in a minute.  Doing this supresses large coefficients and limits the variance of the system---at the cost of increased bias.\n",
    "\n",
    "The following figure illustrates the interaction of the prior and the posterior without the prior:\n",
    "![Ivezic, Figure 8.3](http://www.astroml.org/_images/fig_lasso_ridge_1.png)\n",
    "\n",
    "Scikit-Learn's [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is their implementation of ridge regression, while AstroML implements Ridge Regression as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "X = np.random.random((100,10))\n",
    "y = np.dot(X, np.random.random(10))\n",
    "model = Ridge(alpha=0.05) #alpha here is lambda in the book\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following example compares Gaussian Basis Regression with and without the constraints from Ridge Regression.  It uses 100 evenly spaced Gauassians, which we can see strongly overfits the problem and has very large coefficient values, until a constraint is imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "#from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu = cosmo.distmod(z)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, None], centers, widths)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "classifier = [LinearRegression, Ridge]\n",
    "kwargs = [dict(), dict(alpha=0.005)]\n",
    "labels = ['Gaussian Basis Regression', 'Ridge Regression']\n",
    "\n",
    "for i in range(2):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf.fit(X, mu_sample)\n",
    "    w = clf.coef_\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(221 + i)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        curves = 37 + w * gaussian_basis(z[:, np.newaxis], centers, widths)\n",
    "        curves = curves[:, abs(w) > 0.01]\n",
    "        ax.plot(z, curves,\n",
    "                c='gray', lw=1, alpha=0.5)\n",
    "\n",
    "    ax.plot(z, fit, '-k')\n",
    "    ax.plot(z, mu, '--', c='gray')\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(0.001, 1.8)\n",
    "    ax.set_ylim(36, 52)\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    # plot weights\n",
    "    ax = plt.subplot(223 + i)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$z$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.8)\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-2, 4)\n",
    "    elif i == 2:\n",
    "        ax.set_ylim(-0.5, 2)\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least absolute shrinkage and selection (LASSO) regularization\n",
    "\n",
    "An alternative to Ridge Regression is LASSO, which implies the following contraint\n",
    "\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda |\\theta|.$$\n",
    "\n",
    "This is equivalent to least-squares minimization with \n",
    "$$ |\\theta | < s,$$\n",
    "that is, the penalty is on the absolute values of the regression coefficients, which is also illustrated in Ivezic, Figure 8.3 as shown above.\n",
    "\n",
    "It not only weights the regression coefficients, it also imposes sparsity on the regression\n",
    "model (i.e. the penalty preferentially selects regions of likelihood space that coincide with one of the vertices within the region defined by the regularization).\n",
    "\n",
    "This has the effect of setting one (or more) of the model attributes to zero.  \n",
    "\n",
    "[Scikit-Learn's `LASSO`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "XX = np.random.random((100,10))\n",
    "yy = np.dot(XX, np.random.random(10))\n",
    "model = Lasso(alpha = 0.05)\n",
    "model.fit(XX,yy)\n",
    "y_pred = model.predict(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below I have copied the Ridge regression cell from above.  Modify the three lines of code needed to replace Ridge regression with Lasso regression.  Then experiment with different values of the regularization parameter.\n",
    "\n",
    "N.B.  $\\lambda$ in the book is related to $\\alpha$ in these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge ####\n",
    "\n",
    "#from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu = cosmo.distmod(z)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, None], centers, widths)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "classifier = [LinearRegression, Ridge] ####\n",
    "kwargs = [dict(), dict(alpha=0.005)] ####\n",
    "labels = ['Gaussian Basis Regression', 'Ridge Regression'] ####\n",
    "\n",
    "for i in range(2):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf.fit(X, mu_sample)\n",
    "    w = clf.coef_\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(221 + i)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        curves = 37 + w * gaussian_basis(z[:, np.newaxis], centers, widths)\n",
    "        curves = curves[:, abs(w) > 0.01]\n",
    "        ax.plot(z, curves,\n",
    "                c='gray', lw=1, alpha=0.5)\n",
    "\n",
    "    ax.plot(z, fit, '-k')\n",
    "    ax.plot(z, mu, '--', c='gray')\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(0.001, 1.8)\n",
    "    ax.set_ylim(36, 52)\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    # plot weights\n",
    "    ax = plt.subplot(223 + i)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$z$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.8)\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-2, 4)\n",
    "    elif i == 2:\n",
    "        ax.set_ylim(-0.5, 2)\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you try `alpha=0`?  If not, go ahead and do that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Are you totally confused?  Don't worry, it is much simpler than it seems.  I found [Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html) to be helpful in sorting this out. \n",
    "\n",
    "They write the constraint term as \n",
    "$$\\lambda \\sum_{j=1}^p |\\theta_j|^q,$$\n",
    "which allows us to see that Ridge regression corresponds to $q=2$, while LASSO regression corresponds to $q=1$.  So, they are really the same thing: Bayes estimates with different priors.   The wildly different names are just a nuisance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With that in mind, now let's see if we can understand what is going on in Ivezic, Figure 8.3.  We now have\n",
    "\n",
    "$$ |\\theta |^q < s.$$\n",
    "\n",
    "Think of $s$ as a normalized distance where $s=1$ corresponds to there being no constraint on $\\theta_i$.  Requiring $s<1$ limits the magnitude of $\\theta_i$.  So, in this figure\n",
    "![Ivezic, Figure 8.3](http://www.astroml.org/_images/fig_lasso_ridge_1.png)\n",
    "$s=1$ would make the circle/diamond big enough to include what they call $\\theta_{\\rm normal}$.  \n",
    "\n",
    "It isn't obvious to me, but I guess that the $\\theta_i$ are normalized such that the contraint region is symmetric.  \n",
    "\n",
    "Shrinking $s$ has the effect of adding a prior that moves the best-fit parameters to the intersection of the two sets of contours.  The difference between Ridge and LASSO is just the shape of the constraint region.  For LASSO, the shape is such that some of the parameters may end up being 0.  \n",
    "\n",
    "Figures 3.8 (page 84 of the [Hastie PDF](http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html)), 3.10 (page 89), and 3.12 (page 91) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we choose $\\lambda$?\n",
    "\n",
    "Use cross-validation as discussed last time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's practice using the [Boston Housing data](http://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset).  \n",
    "\n",
    "As a reminder, this data set contains 12 attributes that can be used to predict the price of houses in Boston.  Because the attributes (columns of $X$) are inhomogenous, some may be more relevant than others.  So LASSO might be a good thing to try in such situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell to read in the data\n",
    "#Also identify the index of the \"Number of Rooms\" attribute\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Start by looking at just how the number of rooms predicts the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMindex = np.argwhere(boston.feature_names==\"RM\")[0,0]\n",
    "print(RMindex)\n",
    "\n",
    "X_RM = boston.data[:,RMindex][:,None]\n",
    "y = boston.target\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_RM, y)\n",
    "print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_RM,y)\n",
    "plt.xlabel(\"Number of Rooms\")\n",
    "plt.ylabel(\"House Value (/1000)\")\n",
    "\n",
    "Xgrid = np.linspace(1,9,9)\n",
    "ypred = reg.intercept_ + Xgrid*reg.coef_[0]\n",
    "plt.plot(Xgrid,ypred,c='r')\n",
    "plt.xlim(3.5,9)\n",
    "plt.ylim(0,52)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now use all attributes, first with `LinearRegression`.  Then with LinearRegression again, but with the data scaled using `StandardScaler` from `sklearn.preprocessing` (since the features are very heterogeneous).  Then fit the scaled data agin with LASSO to see which features aren't that important.  We'll plot the coefficients of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import ___\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "___ = StandardScaler()\n",
    "Xscaled = scaler.fit_transform(___)\n",
    "\n",
    "linreg = LinearRegression().fit(___,___)\n",
    "linreg_scaled = LinearRegression().fit(___,___)\n",
    "lasso_scaled = Lasso(alpha=___).fit(___,___)\n",
    "\n",
    "print(linreg.coef_, linreg.intercept_)\n",
    "print(linreg_scaled.coef_, linreg_scaled.intercept_)\n",
    "print(lasso_scaled.coef_, lasso_scaled.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5)) \n",
    "plt.subplots_adjust(hspace=0.001)\n",
    "\n",
    "x_pos = np.arange(len(boston.feature_names))\n",
    "\n",
    "for i in range(3):\n",
    "    # plot theta\n",
    "    ax = plt.subplot(311 + i)\n",
    "    ax.set_ylabel(r'$\\theta$')\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    if i == 2:\n",
    "        ax.set_xticklabels(boston.feature_names, rotation=60)     \n",
    "    else: \n",
    "        ax.set_xticklabels([])\n",
    "    \n",
    "    ax.set_xlim(-0.5, 12.5)\n",
    " \n",
    "    if i == 0:\n",
    "        ax.bar(x_pos, linreg.coef_, alpha=0.5, label=\"LinReg\")\n",
    "        #ax.set_ylim(-2, 4)\n",
    "        plt.legend(loc=4)\n",
    "    elif i == 1:\n",
    "        ax.bar(x_pos, linreg_scaled.coef_, alpha=0.5, label=\"LinRegScaled\")\n",
    "        #ax.set_ylim(-2, 4)\n",
    "        plt.legend(loc=1)\n",
    "    elif i == 2:\n",
    "        ax.bar(x_pos, lasso_scaled.coef_, alpha=0.5, label=\"LASSO\")\n",
    "        #ax.set_ylim(-0.5, 2)\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that NOX had a large coefficient (because it is factor of 10 smaller than RM), but isn't particularly relevant.  So, should probably scale the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Up to now we have been fitting using *linear* models.  Before moving on to *non-linear* models, we'll look at local linear fitting.  Kernel Regression (or Nadaraya-Watson) was one such method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Locally Linear Regression (LOWESS or LOESS)\n",
    "\n",
    "In [Local Linear Regression](https://en.wikipedia.org/wiki/Local_regression) we assume that the regression function at any point can\n",
    "be approximated by a [Taylor series expansion](https://www.mathsisfun.com/algebra/taylor-series.html).  If we truncated the Taylor series at the first term, then this would be the same as Nadaraya-Watson.\n",
    "\n",
    "This is similar to Kernel regression, except that we fit the local regression to the weighted points by finding a $w(x)$ that minimizes\n",
    "\n",
    "$$\\sum_{i=1}^N  K\\left(\\frac{||x-x_i||}{h}\\right) \\left( y_i - w(x) \\, x_i \\right)^2.$$\n",
    "\n",
    "One version of this called LOWESS (locally weighted scatter plot smoothing) uses the \"tricubic\" Kernel:\n",
    "\n",
    "$$K(x_i,x) = \\left ( 1 - \\left ( \\frac{|x - x_i |}h{}\\right )^3 \\right )^3.$$\n",
    "\n",
    "However, the book isn't really very clear on this (or LOESS, which might stand for LOcal regrESSion) and it doesn't appear that this is implemented in either astroML or Scikit-Learn.  So, we are going to move on--just realize that there algorithms that are intermediate between linear and nonlinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear Regression\n",
    "\n",
    "Often we can make our non-linear data linear (e.g., if your $y$ values increase exponentially with $x$, by taking the log), but that has its own set of complications (e.g., asymmetric error bars).  So we should also consider non-linear regression.\n",
    "\n",
    "If we know the theoretical form of the model, then one option is to use MCMC techniques to sample the parameter space and find the optimal model parameters.\n",
    "\n",
    "An alternate approach is to use the Levenberg-Marquardt (LM) algorithm to optimize the maximum likelihood estimation. [Numerical Recipes](http://numerical.recipes/) is an excellent resource for more information about LM.  I can't really emphasize enough how ubiquitous LM is, you really should learn how it works in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now let's leave it as these few words of explanation (with links for further study).\n",
    "LM searches through a combination of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) and [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) optimization. If we can express our\n",
    "regression function as a Taylor series expansion then, to first order,\n",
    "then we can write\n",
    "\n",
    "$$f(x_i|\\theta) = f(x_i|\\theta_0) + J d\\theta.$$\n",
    "\n",
    "Here $\\theta_0$ is an initial guess for the regression parameters,\n",
    "$J$ is the Jacobian about this point ($J=\\partial f(x_i|\\theta)/ \\partial\n",
    " \\theta$), and $d\\theta$ is a perturbation in the regression\n",
    "parameters. \n",
    "\n",
    "LM minimizes the sum of square errors,\n",
    "\n",
    "$$\\sum_i [y_i- f(x_i|\\theta_0) - J_i d\\theta]^2,$$\n",
    "\n",
    "for a perturbation $d\\theta$. This minimization results in an update relation for\n",
    "$d\\theta$ given by\n",
    "\n",
    "$$(J^TC^{-1}J + \\lambda\\ {\\rm diag}(J^TC^{-1}J) )\\,d\\theta = J^TC^{-1}(Y-f(X|\\theta)),$$\n",
    "\n",
    "where $\\lambda$ term acts as a damping parameter.  If $\\lambda$ is small, then the relation approximates a Gauss-Newton method (i.e., it minimizes the parameters assuming the function is quadratic). If $\\lambda$ is large the perturbation $d\\theta$ follows the direction of\n",
    "steepest descent. The diag$(J^TC^{-1}J)$ term is what makes it different from Ridge Regression and it ensures that the update of $d\\theta$ is largest along directions where the gradient is smallest (which\n",
    "improves convergence).\n",
    "\n",
    "This is an iterative process which ceases when the change in likelihood values reaches a predetermined limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Don't worry if that doesn't make any sense.  We are going to talk about gradient descent again when we get to artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In SciPy [`scipy.optimize.leastsq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html) implements the LM algorithm.\n",
    "Here is an example call to estimate the first 6 terms of the Taylor series for $y=\\sin x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "x = np.linspace(-3,3,100) # 100 values between -3 and 3\n",
    "\n",
    "def taylor_err(a, x, f):\n",
    "    p = np.arange(len(a))[:, None]\n",
    "    return f(x) - np.dot(a,x**p)\n",
    "\n",
    "a_start = np.zeros(6) # starting guess\n",
    "a_best, flat = optimize.leastsq(taylor_err, a_start, args=(x,np.sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(a_best) #Print coefficients of the Taylor series exapansion.  Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers\n",
    "\n",
    "To be honest, I'm not at all certain why the book brings up outliers at this particular point.  However, we need to talk about outliers sometime.  As with other things today, we'll skip over a lot and do just enough to give you a feel for what can be done.\n",
    "\n",
    "We'll use what we learned from Chapter 5 to adopt a Bayesian approach to identifying outliers and account for them in our fit.\n",
    "\n",
    "Let's assume the data are drawn from two Gaussians distributions (one for the function and the other for the outliers)\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "&  p(\\{y_i\\}|\\{x_i\\}, \\{\\sigma_i\\}, \\theta_0, \\theta_1, \\mu_b, V_b, p_b)\n",
    "  \\propto \\nonumber\\\\\n",
    "&  \\prod_{i=1}^{N} \\bigg[\n",
    "    \\frac{1-p_b}{\\sqrt{2\\pi\\sigma_i^2}}\n",
    "      \\exp\\left(-\\frac{(y_i - \\theta_1 x_i - \\theta_0)^2}\n",
    "               {2 \\sigma_i^2}\\right)\n",
    "    + \\frac{p_b}{\\sqrt{2\\pi(V_b + \\sigma_i^2)}}\n",
    "    \\exp\\left(-\\frac{(y_i - \\mu_b)^2}{2(V_b + \\sigma_i^2)}\\right)\n",
    "    \\bigg].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$V_b$ is the variance of the outlier distribution. If we use MCMC we can marginalize over the nuisance  parameters $p_b$, $V_b$, $\\mu_b$. We could also calculate the probability that a point is drawn from the outlier or \"model\" Gaussian.\n",
    "\n",
    "![Ivezic, Figure 8.9](https://www.astroml.org/_images/fig_outlier_rejection_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The top-left panel shows the data, including 3 obvious outlier points.  Not accounting for the outliers gives the dotted line and the model parameters shown in the top right.  Accounting for the outliers with two different methods gives the dashed and solid lines in the top left and the parameter fits given in the bottom 2 plots.\n",
    "\n",
    "What is going on here is beyond the scope of what we have time to get into for this class, but I wanted you to be aware that there are tools/methods to handle such cases.  If you want to try this, take a look at the [code for Figure 8.9](https://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html) [GTR 2020: Which currently isn't working for me after the upgrade to pymc3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.9\n",
    "# Author: Jake VanderPlas (adapted to PyMC3 by Brigitta Sipocz)\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from theano import shared as tshared\n",
    "import theano.tensor as tt\n",
    "\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Get data: this includes outliers. We need to convert them to Theano variables\n",
    "data = fetch_hogg2010test()\n",
    "xi = tshared(data['x'])\n",
    "yi = tshared(data['y'])\n",
    "dyi = tshared(data['sigma_y'])\n",
    "size = len(data)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Define basic linear model\n",
    "\n",
    "def model(xi, theta, intercept):\n",
    "    slope = np.tan(theta)\n",
    "    return slope * xi + intercept\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# First model: no outlier correction\n",
    "with pm.Model():\n",
    "    # set priors on model gradient and y-intercept\n",
    "    inter = pm.Uniform('inter', -1000, 1000)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2)\n",
    "\n",
    "    y = pm.Normal('y', mu=model(xi, theta, inter), sd=dyi, observed=yi)\n",
    "\n",
    "    trace0 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Second model: nuisance variables correcting for outliers\n",
    "# This is the mixture model given in equation 17 in Hogg et al\n",
    "def mixture_likelihood(yi, xi):\n",
    "    \"\"\"Equation 17 of Hogg 2010\"\"\"\n",
    "\n",
    "    sigmab = tt.exp(log_sigmab)\n",
    "    mu = model(xi, theta, inter)\n",
    "\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    L_in = (1. / root2pi / dyi * np.exp(-0.5 * (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    L_out = (1. / root2pi / np.sqrt(Vi + Vb)\n",
    "             * np.exp(-0.5 * (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return tt.sum(tt.log((1 - Pb) * L_in + Pb * L_out))\n",
    "\n",
    "\n",
    "with pm.Model():\n",
    "    # uniform prior on Pb, the fraction of bad points\n",
    "    Pb = pm.Uniform('Pb', 0, 1.0, testval=0.1)\n",
    "\n",
    "    # uniform prior on Yb, the centroid of the outlier distribution\n",
    "    Yb = pm.Uniform('Yb', -10000, 10000, testval=0)\n",
    "\n",
    "    # uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "    log_sigmab = pm.Uniform('log_sigmab', -10, 10, testval=5)\n",
    "\n",
    "    inter = pm.Uniform('inter', -200, 400)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2, testval=np.pi / 4)\n",
    "\n",
    "    y_mixture = pm.DensityDist('mixturenormal', logp=mixture_likelihood,\n",
    "                               observed={'yi': yi, 'xi': xi})\n",
    "\n",
    "    trace1 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Third model: marginalizes over the probability that each point is an outlier.\n",
    "# define priors on beta = (slope, intercept)\n",
    "\n",
    "def outlier_likelihood(yi, xi):\n",
    "    \"\"\"likelihood for full outlier posterior\"\"\"\n",
    "\n",
    "    sigmab = tt.exp(log_sigmab)\n",
    "    mu = model(xi, theta, inter)\n",
    "\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    logL_in = -0.5 * tt.sum(qi * (np.log(2 * np.pi * Vi)\n",
    "                                  + (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    logL_out = -0.5 * tt.sum((1 - qi) * (np.log(2 * np.pi * (Vi + Vb))\n",
    "                                         + (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return logL_out + logL_in\n",
    "\n",
    "\n",
    "with pm.Model():\n",
    "    # uniform prior on Pb, the fraction of bad points\n",
    "    Pb = pm.Uniform('Pb', 0, 1.0, testval=0.1)\n",
    "\n",
    "    # uniform prior on Yb, the centroid of the outlier distribution\n",
    "    Yb = pm.Uniform('Yb', -10000, 10000, testval=0)\n",
    "\n",
    "    # uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "    log_sigmab = pm.Uniform('log_sigmab', -10, 10, testval=5)\n",
    "\n",
    "    inter = pm.Uniform('inter', -1000, 1000)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2)\n",
    "\n",
    "    # qi is bernoulli distributed\n",
    "    qi = pm.Bernoulli('qi', p=1 - Pb, shape=size)\n",
    "\n",
    "    y_outlier = pm.DensityDist('outliernormal', logp=outlier_likelihood,\n",
    "                               observed={'yi': yi, 'xi': xi})\n",
    "\n",
    "    trace2 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# plot the data\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.1, top=0.95, hspace=0.2)\n",
    "\n",
    "# first axes: plot the data\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.errorbar(data['x'], data['y'], data['sigma_y'], fmt='.k', ecolor='gray', lw=1)\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Go through models; compute and plot likelihoods\n",
    "linestyles = [':', '--', '-']\n",
    "labels = ['no outlier correction\\n(dotted fit)',\n",
    "          'mixture model\\n(dashed fit)',\n",
    "          'outlier rejection\\n(solid fit)']\n",
    "\n",
    "x = np.linspace(0, 350, 10)\n",
    "\n",
    "bins = [(np.linspace(140, 300, 51), np.linspace(0.6, 1.6, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51))]\n",
    "\n",
    "for i, trace in enumerate([trace0, trace1, trace2]):\n",
    "    H2D, bins1, bins2 = np.histogram2d(np.tan(trace['theta']),\n",
    "                                       trace['inter'], bins=50)\n",
    "    w = np.where(H2D == H2D.max())\n",
    "\n",
    "    # choose the maximum posterior slope and intercept\n",
    "    slope_best = bins1[w[0][0]]\n",
    "    intercept_best = bins2[w[1][0]]\n",
    "\n",
    "    # plot the best-fit line\n",
    "    ax1.plot(x, intercept_best + slope_best * x, linestyles[i], c='k')\n",
    "\n",
    "    # For the model which identifies bad points,\n",
    "    # plot circles around points identified as outliers.\n",
    "    if i == 2:\n",
    "        Pi = trace['qi'].mean(0)\n",
    "        outlier_x = data['x'][Pi < 0.32]\n",
    "        outlier_y = data['y'][Pi < 0.32]\n",
    "        ax1.scatter(outlier_x, outlier_y, lw=1, s=400, alpha=0.5,\n",
    "                    facecolors='none', edgecolors='red')\n",
    "\n",
    "    # plot the likelihood contours\n",
    "    ax = plt.subplot(222 + i)\n",
    "\n",
    "    H, xbins, ybins = np.histogram2d(trace['inter'],\n",
    "                                     np.tan(trace['theta']), bins=bins[i])\n",
    "    H[H == 0] = 1E-16\n",
    "    Nsigma = convert_to_stdev(np.log(H))\n",
    "\n",
    "    ax.contour(0.5 * (xbins[1:] + xbins[:-1]),\n",
    "               0.5 * (ybins[1:] + ybins[:-1]),\n",
    "               Nsigma.T, levels=[0.683, 0.955], colors='black')\n",
    "\n",
    "    ax.set_xlabel('intercept')\n",
    "    ax.set_ylabel('slope')\n",
    "    ax.grid(color='gray')\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(40))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "\n",
    "    ax.text(0.96, 0.96, labels[i], ha='right', va='top',\n",
    "            bbox=dict(fc='w', ec='none', alpha=0.5),\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_xlim(bins[i][0][0], bins[i][0][-1])\n",
    "    ax.set_ylim(bins[i][1][0], bins[i][1][-1])\n",
    "\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(100, 700)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Proccess Regression (aka Fitting by Kindergartners)\n",
    "\n",
    "\n",
    "A [Gaussian Process](https://en.wikipedia.org/wiki/Gaussian_process) (GP) is a collection of random variables in a parameter space for which any subset can be defined by a joint Gaussian distribution.  I have come to think of Gaussian Process Regression as \"fitting by kindergartners\".\n",
    "\n",
    "In the top-left panel below (this is the kindergartner part), we have drawn some random distributions from a Gaussian Basis.  Specifically, we have put down evenly spaced Gaussians across the parameter space, that have width of $h$ and covariance given by \n",
    "\n",
    "$${\\rm Cov}(x_1, x_2; h) = \\exp\\left(\\frac{-(x_1 - x_2)^2}{2 h^2}\\right).$$\n",
    "\n",
    "For a given bandwidth we can obviously define an infinite set of such functions.\n",
    "\n",
    "Then in the top-right panel, we constrain these functions by selecting those that pass though a given set of points using the posterior:\n",
    "\n",
    "$$p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast).$$\n",
    "\n",
    "The bottom panels show the result for the same points with error bars (*bottom left*) and 20 noisy points drawn from $y=\\cos(x)$ (*bottom right*).  You can perhaps see how this might be useful.\n",
    "\n",
    "![Ivezic, Figure 8.10](http://www.astroml.org/_images/fig_gp_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the code that produced that plot (Ivezic, Figure 8.10).  See what happens if you make the number of Gaussians much smaller or much bigger, or if you change the bandwidth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.10\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from scipy.optimize import fmin_cobyla\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a squared exponential covariance function\n",
    "def squared_exponential(x1, x2, h):\n",
    "    return np.exp(-0.5 * (x1 - x2) ** 2 / h ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# draw samples from the unconstrained covariance\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 10, 100) #This sets the number of Gaussians\n",
    "h = 1.0  #This is the Bandwidth\n",
    "\n",
    "mu = np.zeros(len(x))\n",
    "C = squared_exponential(x, x[:, None], h)\n",
    "draws = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two points\n",
    "x1 = np.array([2.5, 7])\n",
    "y1 = np.cos(x1)\n",
    "kernel1 = kernels.RBF(1/0.5, (1/0.5, 1/0.5))\n",
    "gp1 = GaussianProcessRegressor(kernel=kernel1, random_state=0, normalize_y=True)\n",
    "gp1.fit(x1[:, None], y1)\n",
    "f1, f1_err = gp1.predict(x[:, None], return_std=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two noisy points\n",
    "#  scikit-learn gaussian process uses nomenclature from the geophysics\n",
    "#  community, where a \"nugget (alpha parameter)\" can be specified.\n",
    "#  The diagonal of the assumed covariance matrix is multiplied by the nugget.\n",
    "#  This is how the error on inputs is incorporated into the calculation.\n",
    "dy2 = 0.2\n",
    "kernel2 = kernels.RBF(1/0.5, (1/0.5, 1/0.5))\n",
    "gp2 = GaussianProcessRegressor(kernel=kernel2,\n",
    "                               alpha=(dy2 / y1) ** 2, random_state=0)\n",
    "gp2.fit(x1[:, None], y1)\n",
    "f2, f2_err = gp2.predict(x[:, None], return_std=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with many noisy points\n",
    "x3 = np.linspace(0, 10, 20)\n",
    "y3 = np.cos(x3)\n",
    "dy3 = 0.2\n",
    "y3 = np.random.normal(y3, dy3)\n",
    "\n",
    "kernel3 = kernels.RBF(0.5, (0.01, 10.0))\n",
    "gp3 = GaussianProcessRegressor(kernel=kernel3,\n",
    "                               alpha=(dy3 / y3) ** 2, random_state=0)\n",
    "gp3.fit(x3[:, None], y3)\n",
    "f3, f3_err = gp3.predict(x[:, None], return_std=True)\n",
    "\n",
    "# we have fit for the `h` parameter: print the result here:\n",
    "print(\"best-fit theta =\", gp3.kernel_.theta[0])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the diagrams\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "# first: plot a selection of unconstrained functions\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(x, draws.T, '-k')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# second: plot a constrained function\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(x, f1, '-', color='gray')\n",
    "ax.fill_between(x, f1 - 2 * f1_err, f1 + 2 * f1_err, color='gray', alpha=0.3)\n",
    "ax.plot(x1, y1, '.k', ms=6)\n",
    "\n",
    "\n",
    "# third: plot a constrained function with errors\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(x, f2, '-', color='gray')\n",
    "ax.fill_between(x, f2 - 2 * f2_err, f2 + 2 * f2_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x1, y1, dy2, fmt='.k', ms=6)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# third: plot a more constrained function with errors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(x, f3, '-', color='gray')\n",
    "ax.fill_between(x, f3 - 2 * f3_err, f3 + 2 * f3_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x3, y3, dy3, fmt='.k', ms=6)\n",
    "\n",
    "ax.plot(x, np.cos(x), ':k')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For GP regression we want to estimate the value and variance of a new set of points given an input data set. This is equivalent to averaging over all functions that pass through our input data\n",
    "\n",
    "The covariance matrix\n",
    "\n",
    "> $  K = \\begin{pmatrix}\n",
    "    K_{11} & K_{12} \\\\\n",
    "    K_{12}^T & K_{22}\n",
    "  \\end{pmatrix},\n",
    "$\n",
    "\n",
    "where $K_{11}$ is the covariance between the input points $x_i$ with\n",
    "observational errors $\\sigma_i^2$ added in quadrature to the diagonal,\n",
    "$K_{12}$ is\n",
    "the cross-covariance between the input points $x_i$ and the unknown points\n",
    "$x^\\ast_j$, and $K_{22}$ is the covariance between the unknown points\n",
    "$x_j^\\ast$.  Then for observed vectors $\\vec{x}$ and $\\vec{y}$, and a vector\n",
    "of unknown points $\\vec{x}^\\ast$, it can be shown that the posterior is given by\n",
    "\n",
    ">$  p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast) = \\mathcal{N}(\\vec{\\mu}, \\Sigma)$\n",
    "\n",
    "where\n",
    "\n",
    ">$\n",
    "\\begin{eqnarray}\n",
    "  \\vec{\\mu} &=& K_{12} K_{11}^{-1} \\vec{y}, \\\\\n",
    "  \\Sigma &=& K_{22} - K_{12}^TK_{11}^{-1}K_{12}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "$\\mu_j$ gives the expected value $\\bar{f}^\\ast_j$ of the result, and\n",
    "$\\Sigma_{jk}$ gives the error covariance between any two unknown points.\n",
    "\n",
    "_it gives the value and uncertainty of a predicted point_\n",
    "\n",
    "Note that the physics of the underlying process enters through the assumed\n",
    "form of the covariance function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apparently the \"bible\" for GP is [Rasmussen and Williams \"Gaussian Processes for Machine Learning\" (2005)](http://www.gaussianprocess.org/gpml/).\n",
    "\n",
    "The Scikit-Learn [`GaussianProcess`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) implementation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "X = np.random.random((100,2))\n",
    "y = np.sin(10*X[:,0] + X[:,1])\n",
    "gp = GaussianProcessRegressor()\n",
    "gp.fit(X,y)\n",
    "y_pred, dy_pred = gp.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below we see what GP does for the supernova example that we used last time.   What is great is that not only do you get a fit, you get errors and can tell where the fit is good and where it is poor.\n",
    "\n",
    "Gaussian Processes are also pretty useful for time-domain data too, so maybe we'll come back to it after next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.11\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from astropy.cosmology import LambdaCDM\n",
    "\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Generate data\n",
    "cosmo = LambdaCDM(H0=71, Om0=0.27, Ode0=0.73, Tcmb0=0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, cosmo=cosmo)\n",
    "\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# fit the data\n",
    "# Mesh the input space for evaluations of the real function,\n",
    "# the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu ** 2)\n",
    "\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "y_pred, sigma = gp.predict(z_fit[:, None], return_std=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  In this process we are assuming that the $x$ values are error free.  But really they will have error too.  Ivezic $\\S$ 8.8 deals with this, but we are skipping it!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
