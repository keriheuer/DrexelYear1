{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric Density Estimation and Clustering\n",
    "\n",
    "Gordon Richards (2016, 2018, 2020) based on materials from Vanderplas, Leighly, Thibert, and Ivezic 4.4, 6.3, 6.4, 6.6.\n",
    "\n",
    "With asides on [StandardScalar](https://scikit-learn.org/stable/modules/preprocessing.html?highlight=standard%20scalar) and [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Models (GMM)\n",
    "\n",
    "KDE centers each bin (or kernel rather) at each point.  In a [**mixture model**](https://en.wikipedia.org/wiki/Mixture_model) we don't use a kernel for each data point, but rather we fit for the *locations of the kernels*--in addition to the width.  So a mixture model is sort of a hybrid between a tradtional (fixed bin location/size) histogram and KDE.   Using lots of kernels (maybe even more than the BIC score suggests) may make sense if you just want to provide an accurate description of the data (as in density estimation).  Using fewer kernels makes mixture models more like clustering (later today), where the suggestion is still to use many kernels in order to divide the sample into real clusters and \"background\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussians are the most commonly used components for mixture models.  So, the pdf is modeled by a sum of Gaussians:\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $\\alpha_k$ are the \"mixing coefficients\" with $0\\le \\alpha_k \\le 1$ and $\\sum_{k=1}^N \\alpha_k = 1$.\n",
    "\n",
    "We can solve for the parameters using maximum likelihood analyis as we have discussed previously.\n",
    "However, this can be complicated in multiple dimensions, requiring the use of [**Expectation Maximization (EM)**](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) methods.\n",
    "\n",
    "Ivezic Figure 4.2 (next cell) provides an example in 1-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 4.2\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the dataset.\n",
    "#  We'll create our dataset by drawing samples from Gaussians.\n",
    "\n",
    "random_state = np.random.RandomState(seed=1)\n",
    "\n",
    "X = np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Learn the best-fit GaussianMixture models\n",
    "#  Here we'll use scikit-learn's GaussianMixture model. The fit() method\n",
    "#  uses an Expectation-Maximization approach to find the best\n",
    "#  mixture of Gaussians for the data\n",
    "\n",
    "# fit models with 1-10 components\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "#  We'll use three panels:\n",
    "#   1) data + best-fit mixture\n",
    "#   2) AIC and BIC vs number of components\n",
    "#   3) probability that a point came from each component\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3.4))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "# plot 1: data + best-fit mixture\n",
    "ax = fig.add_subplot(131)\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "ax.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(132)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(133)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we have some data that we are trying to describe with a mixture of between 1 and 10 Gaussians.  The figure shows an example where 3 Gaussians provides the best fit.\n",
    "\n",
    "Again, the distribution is modeled as \n",
    "\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "with $0\\le \\alpha_k \\le 1$ controlling the relative high of the Gaussians.  We use Expectation Maximization to determine the properties of each Gaussian ($\\alpha$, $\\mu_k$,and $\\Sigma_k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expectation Maximization (ultra simplified version)\n",
    "\n",
    "(Note: all explanations of EM are far more complicated than seems necessary for our purposes, so here is my overly simplified explanation.)\n",
    "\n",
    "This may make more sense in terms of our earlier Bayesian analyses if we write this as \n",
    "$$p(z=c) = \\alpha_k,$$\n",
    "and\n",
    "$$p(x|z=c) = \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $z$ is a \"hidden\" variable related to which \"component\" each point is assigned to.\n",
    "\n",
    "In the Expectation step, we hold $\\mu_k, \\Sigma_k$, and $\\alpha_k$ fixed and compute the probability that each $x_i$ belongs to component, $c$.  \n",
    "\n",
    "In the Maximization step, we hold the probability of the components fixed and maximize $\\mu_k, \\Sigma_k,$ and $\\alpha_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the following 2-D animation to illustrate the process.  \n",
    "\n",
    "We start with a 2-component GMM, where the initial components can be randomly determined.\n",
    "\n",
    "The points that are closest to the centroid of a component will be more probable under that distribution in the \"E\" step and will pull the centroid towards them in the \"M\" step.  Iteration between the \"E\" and \"M\" step eventually leads to convergence.\n",
    "\n",
    "In this particular example, 3 components better describes the data and similarly converges.  Note that the process is not that sensitive to how the components are first initialized.  We pretty much get the same result in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"B36fzChfyGU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A typical call to the [Gaussian Mixture Model](http://scikit-learn.org/stable/modules/mixture.html) algorithm looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000  points in 2D\n",
    "gmm = GaussianMixture(3) #three components\n",
    "gmm.fit(X)\n",
    "log_dens = gmm.score(X)\n",
    "BIC = gmm.bic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's do the 1-D example given using eruption data from \"Old Faithful\" geyser at Yellowstone National Park.  \n",
    "[http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#eruptions: Eruption time in mins\n",
    "#waiting: Waiting time to next eruption\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/faithful.dat', delim_whitespace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make two \"fancy\" histograms illustrating the distribution of `x=df['eruptions']` and `y=df['waiting']` times.  Use `bins=\"freedman\"` and `histtype=\"step\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as ____\n",
    "from astropy.visualization import hist as ____\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('N')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Waiting')\n",
    "plt.ylabel('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Fit Gaussian Mixtures, first in 1-D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#First fit Eruptions\n",
    "gmm1 = GaussianMixture(n_components=2) # 2-component gaussian mixture model\n",
    "gmm1.fit(df['eruptions'][:,None]) # Fit step\n",
    "xgrid1 = np.linspace(0, 8, 1000) # Make evaluation grid\n",
    "logprob1 = gmm1.score_samples(xgrid1[:,None]) # Compute log likelihoods on that grid\n",
    "pdf1 = np.exp(logprob1)\n",
    "resp1 = gmm1.predict_proba(xgrid1[:,None]) \n",
    "pdf_individual1 = resp1 * pdf1[:, np.newaxis] # Compute posterior probabilities for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Then fit waiting\n",
    "gmm2 = GaussianMixture(n_components=___)\n",
    "gmm2.fit(___)\n",
    "xgrid2 = np.linspace(30, 120, 1000)\n",
    "logprob2 = gmm2.score_samples(___)\n",
    "pdf2 = np.exp(logprob2)\n",
    "resp2 = gmm2.predict_proba(___)\n",
    "pdf_individual2 = ___ * ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make plots\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "plt.hist(df['eruptions'], bins=6, density=True, histtype='step')\n",
    "plt.plot(xgrid1, pdf_individual1, '--', color='blue')\n",
    "plt.plot(xgrid1, pdf1, '-', color='gray')\n",
    "plt.xlabel(\"Eruptions\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "plt.hist(____, bins=9, ____, ___)\n",
    "plt.plot(___, ____, '--', color='blue')\n",
    "plt.plot(___, ____, '-', color='gray')\n",
    "plt.xlabel(\"Waiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now do a more complicated 1-D example (Ivezic, Figure 6.8), which compares a Mixture Model to KDE.\n",
    "[Note that the version at astroML.org has some bugs!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.8, modified by GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from astropy.visualization import hist\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "np.random.seed(0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N))\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "np.random.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(bottom=0.08, top=0.95, right=0.95, hspace=0.1)\n",
    "N_values = (500, 5000)\n",
    "subplots = (211, 212)\n",
    "k_values = (10, 100)\n",
    "\n",
    "for N, k, subplot in zip(N_values, k_values, subplots):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    xN = x[:N]\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "\n",
    "    kde = KernelDensity(0.1, kernel='gaussian')\n",
    "    kde.fit(xN[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(t[:, None]))\n",
    "\n",
    "    # Compute density via Gaussian Mixtures\n",
    "    # we'll try several numbers of clusters\n",
    "    n_components = np.arange(3, 20)\n",
    "    gmms = [GaussianMixture(n_components=n).fit(xN[:,None]) for n in n_components]\n",
    "    BICs = [gmm.bic(xN[:,None]) for gmm in gmms]\n",
    "    i_min = np.argmin(BICs)\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "    logprob = gmms[i_min].score_samples(t[:,None])\n",
    "\n",
    "    # plot the results\n",
    "    ax.plot(t, true_pdf(t), ':', color='black', zorder=3,\n",
    "            label=\"Generating Distribution\")\n",
    "    ax.plot(xN, -0.005 * np.ones(len(xN)), '|k', lw=1.5)\n",
    "    ax.plot(t, np.exp(logprob), '-', color='gray',\n",
    "            label=\"Mixture Model\\n(%i components)\" % n_components[i_min])\n",
    "    ax.plot(t, dens_kde, '-', color='black', zorder=3,\n",
    "            label=\"Kernel Density $(h=0.1)$\")\n",
    "\n",
    "    # label the plot\n",
    "    ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    if subplot == 212:\n",
    "        ax.set_xlabel('$x$')\n",
    "\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the BIC values and see why it picked that many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.scatter(n_components,BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do the individual components look like?  Make a plot of those.  Careful with the shapes of the arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(gmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# See Ivezic, Figure 4.2 for help: http://www.astroml.org/book_figures/chapter4/fig_GMM_1D.html\n",
    "# The index \"8\" is choosing the instance with 11 components.\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "print(len(gmms[8].weights_))\n",
    "logprob =  gmms[8].score_samples(t[:,None])\n",
    "pdf = np.exp(logprob) # Sum of the individual component pdf\n",
    "resp = gmms[8].predict_proba(t[:,None]) # Array of \"responsibilities\" for each component\n",
    "pdf_individual = resp*pdf[:,None]\n",
    "plt.plot(t,pdf_individual)\n",
    "plt.xlim((0,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's look at the Old Faithful data again, but this time in 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll fit both features at the same time (i.e., the $x$ and $y$ axes above) with `n_components=2`.  Note that Scikit-Learn can handle Pandas DataFrames without further conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gmm3 = GaussianMixture(____=____)\n",
    "gmm3.fit(df[[____,____]]) #Note no need for \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the components have been fit, we can plot the location of the centroids and the \"error\" ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Kludge to fix the bug with draw_ellipse in astroML v1.0\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(mu, C, scales=[1, 2, 3], ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # find principal components and rotation angle of ellipse\n",
    "    sigma_x2 = C[0, 0]\n",
    "    sigma_y2 = C[1, 1]\n",
    "    sigma_xy = C[0, 1]\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy,\n",
    "                             (sigma_x2 - sigma_y2))\n",
    "    tmp1 = 0.5 * (sigma_x2 + sigma_y2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x2 - sigma_y2) ** 2 + sigma_xy ** 2)\n",
    "\n",
    "    sigma1 = np.sqrt(tmp1 + tmp2)\n",
    "    sigma2 = np.sqrt(tmp1 - tmp2)\n",
    "\n",
    "    for scale in scales:\n",
    "        ax.add_patch(Ellipse((mu[0], mu[1]),\n",
    "                             2 * scale * sigma1, 2 * scale * sigma2,\n",
    "                             alpha * 180. / np.pi,\n",
    "                             **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#See cell above\n",
    "#from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])\n",
    "\n",
    "ax.scatter(gmm3.means_[:,0], gmm3.means_[:,1], marker='s', c='red', s=80)\n",
    "for mu, C, w in zip(gmm3.means_, gmm3.covariances_, gmm3.weights_):\n",
    "    draw_ellipse(mu, 2*C, scales=[1], ax=ax, fc='none', ec='k') #2 sigma ellipses for each component\n",
    "#Add a 3-sigma error ellipse    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 6.6 shows another 2-D example.  In the first panel, we have the raw data.  We then try to represent the data with a series of Gaussians.  We allow up to 14 Gaussians and use the AIC/BIC to determine the best choice for this number.  This is shown in the second panel.  Finally, the third panel shows the chosen Gaussians with their centroids and 1-$\\sigma$ contours on top of a density plot of our data.\n",
    "\n",
    "In this case 5 components are required for the best fit.  While it looks like we could do a pretty good job with just 2 components, there does appear to be some \"background\" that is a high enough level to justify further components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.6\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "from astroML.utils.decorators import pickle_results\n",
    "#See cells above\n",
    "#from astroML.plotting import draw_ellipse\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the Segue Stellar Parameters Pipeline data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute GaussianMixture models & AIC/BIC\n",
    "N = np.arange(1, 14)\n",
    "\n",
    "\n",
    "@pickle_results(\"GMM_metallicity.pkl\")\n",
    "def compute_GaussianMixture(N, covariance_type='full', max_iter=1000):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        print(N[i])\n",
    "        models[i] = GaussianMixture(n_components=N[i], max_iter=max_iter,\n",
    "                                    covariance_type=covariance_type)\n",
    "        models[i].fit(X)\n",
    "    return models\n",
    "\n",
    "models = compute_GaussianMixture(N)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print(\"best fit converged:\", gmm_best.converged_)\n",
    "print(\"BIC: n_components =  %i\" % N[i_best])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute 2D density\n",
    "FeH_bins = 51\n",
    "alphFe_bins = 51\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'],\n",
    "                                          (FeH_bins, alphFe_bins))\n",
    "\n",
    "Xgrid = np.array(list(map(np.ravel,\n",
    "                          np.meshgrid(0.5 * (FeH_bins[:-1]\n",
    "                                             + FeH_bins[1:]),\n",
    "                                      0.5 * (alphFe_bins[:-1]\n",
    "                                             + alphFe_bins[1:]))))).T\n",
    "log_dens = gmm_best.score_samples(Xgrid).reshape((51, 51))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "fig.subplots_adjust(wspace=0.45,\n",
    "                    bottom=0.25, top=0.9,\n",
    "                    left=0.1, right=0.97)\n",
    "\n",
    "# plot density\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Input\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(132)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, ':k', label='BIC')\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(np.exp(log_dens),\n",
    "          origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "ax.scatter(gmm_best.means_[:, 0], gmm_best.means_[:, 1], c='w')\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covariances_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[1.5], ax=ax, fc='none', ec='k')\n",
    "\n",
    "ax.text(0.93, 0.93, \"Converged\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Talk about how to use this to do outlier finding.  Convolve with errors of unknown object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly, let's look at a 2-D case where we are using GMM more to characterize the data than to find clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 6.7\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create a function which will save the results to a pickle file\n",
    "#  for large number of clusters, computation will take a long time!\n",
    "@pickle_results('great_wall_GMM.pkl')\n",
    "def compute_GMM(n_clusters, max_iter=1000, tol=3, covariance_type='full'):\n",
    "    clf = GaussianMixture(n_clusters, covariance_type=covariance_type,\n",
    "                          max_iter=max_iter, tol=tol, random_state=0)\n",
    "    clf.fit(X)\n",
    "    print(\"converged:\", clf.converged_)\n",
    "    return clf\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a grid on which to evaluate the result\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#\n",
    "# we'll use 100 clusters.  In practice, one should cross-validate\n",
    "# with AIC and BIC to settle on the correct number of clusters.\n",
    "clf = compute_GMM(n_clusters=100)\n",
    "log_dens = clf.score_samples(Xgrid).reshape(Ny, Nx)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(211, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(212, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that this is very different than the non-parametric density estimates.  The advantage is that we now have a *model*.  This model can be stored very compactly with just a few numbers, unlike the KDE or KNN maps which require a floating point number for each grid point.  \n",
    "\n",
    "One thing that you might imagine doing with this is subtracting the model from the data and looking for interesting things among the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) algorithms attempt to group together like objects in a data set.  This process allows us to put new objects into the resulting classes and to identify rare objects that do not fit any particular mold.  Clustering is inherently an \"unsupervised\" process as we do not know the classification of the objects.  Since we have no metric for determining when we are right, it is a bit of a dark art, but it also can be very powerful.  Scikit-Learn's clustering suite is summarized at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-Means Clustering\n",
    "\n",
    "We start with [$K$-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), which is one of the simplest methods.  $K$-means seeks to minimize the following\n",
    "\n",
    "$$\\sum_{k=1}^{K}\\sum_{i\\in C_k}||x_i - \\mu_k||^2$$\n",
    "\n",
    "where $\\mu_k = \\frac{1}{N_k}\\sum_{i\\in C_k} x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This says to\n",
    "  * Take every object in class $C_k$ (as determined by which centroid it is closest to, specifically $C_k(x_i) = \\arg \\min_k ||x_i-\\mu_k||)$\n",
    "  * Compute the mean of the objects in that class\n",
    "  * Subtract that mean from each member of that class and square the norm\n",
    "  * Do that for each class and sum\n",
    "  * Shift the centroids of the *pre-determined* number of classes until this sum is minimized\n",
    "  * Do this multiple times with different starting centroids and take the result with the minimum sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A typical call will look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000 points in 2D\n",
    "clf = KMeans(n_clusters=3) #Try 3 clusters to start with\n",
    "clf.fit(X)\n",
    "centers=clf.cluster_centers_ #location of the clusters\n",
    "labels=clf.predict(X) #labels for each of the points\n",
    "\n",
    "# To get some information on these try:\n",
    "# KMeans?\n",
    "# help(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example with the same data that we used for GMM.  Note how the background shifts the centroids from what you might expect.  So, the mixture model might work better in this case.\n",
    "\n",
    "However, one might consider running the K-means algorithm in order to find a suitable initialization for GMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 6.13\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a 2D histogram  of the input\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], 50)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the KMeans clustering\n",
    "n_clusters = 4\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# plot density\n",
    "ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "FeH_centers = 0.5 * (FeH_bins[1:] + FeH_bins[:-1])\n",
    "alphFe_centers = 0.5 * (alphFe_bins[1:] + alphFe_bins[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(FeH_centers, alphFe_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "#H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "#\n",
    "#for i in range(n_clusters):\n",
    "#    Hcp = H.copy()\n",
    "#    flag = (Hcp == i)\n",
    "#    Hcp[flag] = 1\n",
    "#    Hcp[~flag] = 0\n",
    "#\n",
    "#    ax.contour(FeH_centers, alphFe_centers, Hcp, [-0.5, 0.5],\n",
    "#               linewidths=1, colors='k')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A few things to note\n",
    "* This was supposed to show the boundaries between the clusters, but it isn't working.  See another example below.\n",
    "* We scaled the data (subtracted the mean and scaling to unit variance) using `StandardScaler()` before running K-Means\n",
    "* We had to *un*scale the data to plot the centers\n",
    "* Plotting the cluster boundaries is not straightforward, but this gives you an example to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's do an example with unscaled then scaled data so you can see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "#Make two blobs with 3 features\n",
    "X,y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Now make one of those features have a radically different scale\n",
    "X[:,0] = X[:,0]+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make a scaled version of X (subtract the mean and divide by the standard deviation)\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X_scaled[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Plot the unscaled and scaled data\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=y)\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Do KMeans clustering with 2 clusters on the scaled data.\n",
    "from sklearn.cluster import ____\n",
    "\n",
    "clf = KMeans(____=____) #Make 2 clusters to start with\n",
    "clf.fit(____)\n",
    "centers=clf.cluster_centers_ #location of the clusters\n",
    "labels=clf.predict(____) #labels for each of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Plot the unscaled and scaled data and the centers that we just computed\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "ax[0].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')\n",
    "\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=labels)\n",
    "ax[1].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because the centers were computed with the scaled data, they don't correspond to the original (unscaled) data points. But we can do an inverse transform if we want to be able to plot the data in its natural units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Unscale the centers and try again\n",
    "centers_unscaled = scaler.inverse_transform(centers)\n",
    "\n",
    "#Plot the unscaled and scaled data\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "ax[0].scatter(centers_unscaled[:, 0], centers_unscaled[:, 1], s=150, c='red', edgecolors='k')\n",
    "\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=labels)\n",
    "ax[1].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that because of the standardization enabled by the Scikit-Learn API, it is easy to chain operations together into a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  See below for a very simple example of how to do the same thing that we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=2))])\n",
    "                 \n",
    "pipe.fit(X)\n",
    "centers=pipe['kmeans'].cluster_centers_ #location of the clusters\n",
    "labels=pipe.predict(X) #labels for each of the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly, K-means is great for idealized data (roughly spherical clusters of similar density), it has some problems with more complicated situations.  The following situation may not be realistic, but it illustrates where we can run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Create two samples that are not spherically symmetric and try Kmeans.\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "____ = KMeans(n_clusters=2) #Try 2 clusters as there are clearly 2 by eye\n",
    "clf.____(____)\n",
    "centers=clf.____ #location of the clusters\n",
    "labels=clf.____ #labels for each of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1.0, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not so great.  \n",
    "\n",
    "Let's try another algorithm, [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html), which is \"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".  This isn't discussed in the Ivezic book, but the link will take you to the description given in Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that DBSCAN is *very* sensitive to these parameters.  Here we'll just do trial and error.  Try a few values of `eps` between 0.05 and 0.2 and `min_samples` between 3 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_dbscan(dbscan, X, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not only is DBSCAN telling you which cluster each object belongs to, it is also \"outliers\" (denoted by the red crosses), which is another important use of clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example from Prof. Cruz, showing K-means boundaries (which were not working in our example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the next few cells\n",
    "from astropy.table import Table\n",
    "t = Table.read('../data/cruz_all_dist.dat', format=\"ascii\")\n",
    "\n",
    "# Just something that you should know that you can do\n",
    "t[::10000].show_in_notebook(display_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Turn these data into a properly formatted Scikit-Learn array\n",
    "X = np.vstack([ t['col2'], t['col3'], t['col4'], t['col5'] ]).T\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Project onto 2 axes with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 2 components\n",
    "pca.fit(X) # Do the fitting\n",
    "\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], marker=\".\", color='k', edgecolors='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the KMeans clustering\n",
    "n_clusters = 6\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make some plots\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Compute a 2D histogram  of the input\n",
    "H, xedges, yedges = np.histogram2d(X_reduced[:,0], X_reduced[:,1], 50)\n",
    "\n",
    "# plot density\n",
    "#ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[xedges[0], xedges[-1],\n",
    "                  yedges[0], yedges[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "x_centers = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "y_centers = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(x_centers, y_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(x_centers, y_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=1, colors='k')\n",
    "\n",
    "    \n",
    "    H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "    \n",
    "#ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(xedges[0], xedges[-1])\n",
    "ax.set_ylim(yedges[0], yedges[-1])\n",
    "\n",
    "ax.set_xlabel('Eigenvalue 1')\n",
    "ax.set_ylabel('Eigenvalue 2')\n",
    "\n",
    "#plt.savefig('cruz.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "In [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), we don't specify the number of clusters ahead of time, we start with $N$ clusters representing each data point.  Then the most similar clusters are joined together, the process repeating until some threshhold is reached.  Actually the process can go in the other direction as well.  What results is called a *dendrogram*, an example of which is shown below.\n",
    "\n",
    "![](https://onlinecourses.science.psu.edu/stat505/sites/onlinecourses.science.psu.edu.stat505/files/lesson12/dendogram_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Clusters are merged at each step according to which are \"nearest\" to each other---where the definition of nearest needs to be specified.  A typical choice results in what is called a \"minimum spanning tree\" (which can be quite slow for large data sets).  Some threshhold needs to be specified to tell the process where to stop (e.g., we are going to treat the green and red objects in the example above as separate clusters).  \n",
    "\n",
    "![Here is an illustration](https://www.statisticshowto.com/wp-content/uploads/2016/11/clustergram.png)\n",
    "\n",
    "Below is an example call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X = np.random.random((1000,2))\n",
    "G = kneighbors_graph(X, n_neighbors=10, mode='distance')\n",
    "T = minimum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, but that's all that the book give us.  There is nothing about what to do with `G` and `T`.  So, instead I'm going to show you a really cool example from a colleague.  In this example Nathalie Thibert is taking spectroscopic data of a certain sub-class of quasars.  She is then grouping the objects into \"like\" bins using a hierarchical clustering algorithm.  The code below is based on the [scipy implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) and takes us through both the analysis and visualization of the data.  It makes use of the [Python Data Analysis Library (pandas)](http://pandas.pydata.org/) and ['pickled'](https://docs.python.org/2/library/pickle.html) \n",
    "data, the latter of which we have not talked about.\n",
    "\n",
    "For another detailed example of hierarchical clustering, see [https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load code/thibert_cluster1.py\n",
    "## Hierarchical Clustering Example: BAL Quasar UV Spectra (pre-reduced)\n",
    "## Author: Nathalie C. M. Thibert (Saint Mary's University), modified from\n",
    "## \t   code by Mark Daley (Western University)\n",
    "##\n",
    "## Method: Agglomerative Hierarchical Clustering\n",
    "## Distance Metric: Complete Linkage\n",
    "## Data: 100 BAL Quasar UV Spectra over ~1400-1550 Ang (i.e., the C IV BAL) \n",
    "##\t Spectra are already in rest-frame, normalized to the local continuum \n",
    "## \t and emission lines, and resampled to a common wavelength grid. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# import clustering algorithms from scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Import pickled BAL quasar data.\n",
    "#data = pd.DataFrame(pd.read_pickle('../data/balquasar_data.pkl')) # Should have 500 wavelength values and 100 spectra.\n",
    "data = pd.read_pickle('../data/balquasar_data_new.pkl') # Should have 500 wavelength values and 100 spectra.\n",
    "\n",
    "\n",
    "# Over plot some example spectra\n",
    "wl = np.arange(1400.1,1549.8,0.3)\n",
    "spec0 = data.T.iloc[0] # You can change the index to see different spectra (choose 0,1,2,...,99).\n",
    "spec5 = data.T.iloc[5]\n",
    "spec7 = data.T.iloc[7]\n",
    "plt.figure() \n",
    "plt.plot(wl,spec5)\n",
    "plt.plot(wl,spec0)\n",
    "plt.plot(wl,spec7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load code/thibert_cluster2.py\n",
    "## Hierarchical Clustering Example: BAL Quasar UV Spectra (pre-reduced)\n",
    "## Author: Nathalie C. M. Thibert (Saint Mary's University), modified from\n",
    "## \t   code by Mark Daley (Western University)\n",
    "##\n",
    "## Method: Agglomerative Hierarchical Clustering\n",
    "## Distance Metric: Complete Linkage\n",
    "## Data: 100 BAL Quasar UV Spectra over ~1400-1550 Ang (i.e., the C IV BAL) \n",
    "##\t Spectra are already in rest-frame, normalized to the local continuum \n",
    "## \t and emission lines, and resampled to a common wavelength grid. \n",
    "\n",
    "\n",
    "# Compute Pearson correlation matrix for 100 spectra. \n",
    "# Each element is a pairwise comparison b/w two spectra.\n",
    "c = data.corr() # Should have 100 rows and 100 columns.\n",
    "\n",
    "# Compute absolute-valued Pearson distance matrix.\n",
    "dp = 1.0 - np.abs(c)\n",
    "\n",
    "# Compute Euclidean distance matrix for the first dendrogram\n",
    "de1 = squareform(pdist(dp,metric='euclidean')) \n",
    "\n",
    "# Do it again for the second dendrogram\n",
    "de2 = squareform(pdist(dp.T,metric='euclidean'))\n",
    "\n",
    "# Start the dendrogram plot.\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Add the first dendrogram (on the left side)\n",
    "ax1 = f.add_axes([0.09, 0.1, 0.2, 0.6])\n",
    "Y = linkage(de1, method='complete') # This is where the hierarchical clustering takes place.\n",
    "Z1 = dendrogram(Y, orientation='left',show_leaf_counts=False, no_labels=True) # Plots dendrogram.\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Add the second dendrogram (on the top)\n",
    "ax2 = f.add_axes([0.3, 0.71, 0.6, 0.2])\n",
    "Y = linkage(de2, method='complete')\n",
    "Z2 = dendrogram(Y,show_leaf_counts=False, no_labels=True)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "# Add the (main) plot of the (clustered) Euclidean distance matrix.\n",
    "axmatrix = f.add_axes([0.3, 0.1, 0.6, 0.6])\n",
    "idx1 = Z1['leaves']\n",
    "idx2 = Z2['leaves']\n",
    "D = de1[idx1, :]\n",
    "D = D[:, idx2]\n",
    "im = axmatrix.matshow(D, aspect='auto', origin='lower', cmap='hot')\n",
    "axmatrix.set_xticks([])\n",
    "axmatrix.set_yticks([])\n",
    "    \n",
    "axcolor = f.add_axes([0.91,0.1,0.02,0.6])\n",
    "pylab.colorbar(im,cax=axcolor)\n",
    "f.show()\n",
    "\n",
    "## NOTE: The colours in the dendrograms correspond to a flat clustering given \n",
    "##\t the default distance threshold in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the side and top dendrograms are the same data.  It is just that the 2-D visualization better lets us see what groups go together.\n",
    "\n",
    "I don't pretend to fully understand each step of this process, but the end result is really cool and I think that there is enough here to get you started if we were interested in trying to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary of Chapter 6 methods from Ivezic Table 6.1 \n",
    "\n",
    "|Method          |Accuracy|Interpretability|Simplicity|Speed|\n",
    "|----------------|--------|----------------|----------|-----|\n",
    "|K-nearest Neighbor| H | H | H | M |\n",
    "|Kernel Density Estimation| H | H | H | H |\n",
    "|Gaussian Mixture Models| H | M | M | M |\n",
    "|Extreme Deconvolution| H | H | M | M |\n",
    "||||||\n",
    "|K-Means| L | M | H | M |\n",
    "|Max-radius minimization| L | M | M | M |\n",
    "|Mean shift| M | H | H | M |\n",
    "|Hierarchical Clustering| H | L | L | L |"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
