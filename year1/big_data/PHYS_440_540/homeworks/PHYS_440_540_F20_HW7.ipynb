{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 7: Problems\n",
    "## Due Friday 13 November, by 4:59pm\n",
    "\n",
    "### PHYS 440/540, Fall 2020\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problems 1&2\n",
    "\n",
    "Fill in the blanks in the code cells below.  Where I have asked a questions that requires an answer in words, I'm just looking for ~1 sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 2-D data set that is close to, but isn't quite linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=500\n",
    "D=2\n",
    "\n",
    "X = 10*np.random.random((____,____)) # N points in D dimensions\n",
    "dy = np.random.normal(loc=0,scale=1,size=____) # add heteroscedastic errors\n",
    "#Simulate a distribution that isn't quite linear, but close.  Adding some noise\n",
    "y = 50 + 0.5*X[:,0]**2 + -0.8*X[:,1]**2 + dy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what the data look like.  It is 2-D so we need 2, 1-D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "ax[0].scatter(X[:,0],y,s=50)\n",
    "ax[0].set_xlabel('X[0]')\n",
    "ax[0].set_ylabel('y')\n",
    "\n",
    "ax[1].scatter(X[:,1],y,s=50)\n",
    "ax[1].set_xlabel('X[1]')\n",
    "ax[1].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try fitting it with plain vanilla linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = ____()\n",
    "____.fit(X, y)\n",
    "intercept = ____.intercept_\n",
    "slopes = ____.coef_\n",
    "print(intercept,slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of those trends.  We first need a grid of X values for the sake of plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgrid0 = np.linspace(X[:,0].min(),X[:,0].max(),50)\n",
    "ypred0 = slopes[0]*Xgrid0 + intercept\n",
    "\n",
    "Xgrid1 = np.linspace(X[:,1].min(),X[:,1].max(),50)\n",
    "ypred1 = slopes[1]*Xgrid1 + intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data and the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig,ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "ax[0].scatter(X[:,0],y,s=50)\n",
    "ax[0].plot(Xgrid0,ypred0,\"r-\")\n",
    "ax[0].set_xlabel('X[0]')\n",
    "ax[0].set_ylabel('y')\n",
    "\n",
    "ax[1].scatter(X[:,1],y,s=50)\n",
    "ax[1].plot(Xgrid1,ypred1,\"r-\")\n",
    "ax[1].set_xlabel('X[1]')\n",
    "ax[1].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't the lines go through the data?  The slopes look about right, but not the intercepts...\n",
    "\n",
    "**Put 1-sentence answer here**\n",
    "\n",
    "We can solve this problem with a 3-D plot showing both dimensions of X.\n",
    "\n",
    "**Note that the 3-D plotting will NOT work in Jupyter Lab.  Just Jupyter Notebooks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu, vv = np.meshgrid(Xgrid0, Xgrid1)\n",
    "Xgrid = np.array([uu.flatten(), vv.flatten()]).T\n",
    "print(Xgrid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = linreg.____(Xgrid)\n",
    "print(ypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes the plot interactive.  You can rotate it\n",
    "#Sometimes I have to run this cell twice\n",
    "%matplotlib notebook  \n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypred, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn off \"notebook\" plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a D dimension polynomial regression.  See what 4th order does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import PolynomialRegression\n",
    "order = 4\n",
    "poly = ____(order) # fit Nth order polynomial\n",
    "poly.____(X,y)\n",
    "ypredpoly = poly.____(Xgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredpoly, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  Note that it does the polynomial regression in both dimensions of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do a Nadaraya-Watson regression on the data.  First with `h=0.2` as the bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import ____\n",
    "nwreg = NadarayaWatson(kernel='gaussian',____=____)\n",
    "nwreg.fit(X,y)\n",
    "ypredNW = nwreg.predict(Xgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredNW, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that is kind of lumpy.  We could try different values of the bandwidth to make it smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was going to have you do a gridSearchCV that included all of those to find the best fit parameters of each and the best overall.  However, there seem to be some bugs with doing that the simple way (related to Python 3 and or the differences in API between sklearn and astroML?).  \n",
    "\n",
    "But here is a link describing one way to do gridSearchCV across multiple models (which don't all have the same parameters:\n",
    "http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "\n",
    "For now, we'll just run gridSearchCV to determine the best Ridge regression parameters.  We'll try two different \"solvers\" as well, just so you can see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_params = {\n",
    "'alpha': [0.05,0.1,0.2,0.5],\n",
    "'solver': ['svd', 'lsqr']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeGrid = GridSearchCV(Ridge(), param_grid = ridge_params)\n",
    "ridgeGrid.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the `best_params_` and the `best_score_` that goes with it?  Make a 3-D plot with those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridgeGrid.____)\n",
    "print(ridgeGrid.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the regression with those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeReg = Ridge(____=____,____=____)\n",
    "ridgeReg.fit(X,y)\n",
    "ypredRidge = ridgeReg.predict(Xgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredRidge, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Ridge Regression in this case doesn't really make sense.   Why not?  \n",
    "\n",
    "**Put ~1 sentence answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try Gaussian Process Regression.  Note that this also returns the uncertainty on $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gp = GaussianProcessRegressor()\n",
    "gp.fit(X,y)\n",
    "ypredGP, dypredGP = gp.predict(____, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredGP, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "ax.set_zlim(0,200)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa!  In class, I made it sound like GP was the best solution.  So, what's going on here?  A bad choice of kernel for one (which we haven't talked about), but what else?\n",
    "\n",
    "\n",
    "**Put 1-sentence answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for these data, the PolynomialRegression seems best.  Let's figure out the best fit model, the coefficients of that model and how well we might expect to do with unknown data.\n",
    "\n",
    "First do a train-test split with a `test_size` of 20%.  Also split out a cross-validation set from the training set (leaving a somewhat smaller set to use for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrainall, Xtest, ytrainall, ytest = train_test_split(X, y, ____=____, random_state=42)\n",
    "\n",
    "Xtrain, Xcv, ytrain, ycv = train_test_split(____,____, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PolynomialRegression\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "degrees = np.arange(1, 12)\n",
    "training_err = np.zeros(degrees.shape)\n",
    "crossval_err = np.zeros(degrees.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i,d in enumerate(degrees):\n",
    "    #print(i,d)\n",
    "    poly = PolynomialRegression(degree=d)\n",
    "\n",
    "    poly.fit(Xtrain,ytrain)\n",
    "    \n",
    "    ypredTrain = poly.predict(____)\n",
    "    ypredCV = poly.predict(____)\n",
    "    \n",
    "    training_err[i] = np.sqrt(np.sum((____ - ytrain) ** 2)/len(ytrain))\n",
    "    \n",
    "    crossval_err[i] = np.sqrt(np.sum((____ - ycv) ** 2)/len(ycv))\n",
    "    \n",
    "\n",
    "BIC_train = np.sqrt(len(y)) * training_err + degrees * np.log(len(y))\n",
    "BIC_crossval = np.sqrt(len(y)) * crossval_err + degrees * np.log(len(y))\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(degrees, crossval_err, '--k', label='cross-validation')\n",
    "ax.plot(degrees, training_err, '-k', label='training')\n",
    "#ax.plot(degrees, 0.1 * np.ones(degrees.shape), ':k')\n",
    "\n",
    "ax.set_xlim(0, 12)\n",
    "\n",
    "#ax.set_xlim(0, 14)\n",
    "#ax.set_ylim(0, 0.8)\n",
    "\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(degrees, BIC_crossval, '--k', label='cross-validation')\n",
    "ax.plot(degrees, BIC_train, '-k', label='training')\n",
    "\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 200)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('BIC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best fit model to predict the error on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialRegression(____)\n",
    "poly.fit(____,____)\n",
    "\n",
    "ypredTest = poly.predict(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err = np.sqrt(np.sum((ypredTest - ytest) ** 2)/len(ytest))\n",
    "print(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the training error for that model?  What does that mean in terms of the importance of doing train-test splitting?\n",
    "\n",
    "**Put ~1 sentence answer here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
