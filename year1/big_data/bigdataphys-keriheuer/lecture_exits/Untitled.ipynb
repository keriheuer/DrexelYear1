{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Autoencoder for SDSS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SDSS example from lecture\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "\n",
    "# from sklearn.metrics import roc_curve\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "# import random\n",
    "\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# if \"setup_text_plots\" not in globals():\n",
    "#     from astroML.plotting import setup_text_plots\n",
    "# setup_text_plots(fontsize=8, usetex=True)\n",
    "# plt.rcParams['axes.xmargin'] = 0.05\n",
    "# plt.rcParams['axes.ymargin'] = 0.05\n",
    "\n",
    "\n",
    "# def read_savefile(filename):\n",
    "#     '''Read npy save file containing images or labels of galaxies'''\n",
    "#     return np.load(filename)\n",
    "\n",
    "\n",
    "# def CNN(img_channels, img_rows, img_cols, verbose=False):\n",
    "#     '''Define CNN model for Nair and Abraham data'''\n",
    "\n",
    "#     # some hyperparamters you can chage\n",
    "#     dropoutpar = 0.5\n",
    "#     nb_dense = 64\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Convolution2D(32, 6, 6, border_mode='same',\n",
    "#                             input_shape=(img_rows, img_cols, img_channels)))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "#     model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Convolution2D(128, 2, 2, border_mode='same'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(nb_dense, activation='relu'))\n",
    "#     model.add(Dropout(dropoutpar))\n",
    "#     model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "#     print(\"Compilation...\")\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "#     print(\"... done!\")\n",
    "#     if verbose is True:\n",
    "#         print(\"Model Summary\")\n",
    "#         print(\"===================\")\n",
    "#         model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def train_CNN(X, Y, ntrain, nval, output=\"test\", verbose=False):\n",
    "#     '''Train the CNN given a dataset and output model and weights'''\n",
    "\n",
    "#     # train params - hardcoded for simplicity\n",
    "#     batch_size = 30\n",
    "#     nb_epoch = 50\n",
    "#     data_augmentation = True  # if True the data will be augmented at every iteration\n",
    "\n",
    "#     ind = random.sample(range(0, ntrain+nval-1), ntrain+nval-1)\n",
    "#     X_train = X[ind[0:ntrain], :, :, :]\n",
    "#     X_val = X[ind[ntrain:ntrain+nval], :, :, :]\n",
    "#     Y_train = Y[ind[0:ntrain]]\n",
    "#     Y_val = Y[ind[ntrain:ntrain+nval]]\n",
    "\n",
    "#     # input image dimensions\n",
    "#     img_rows, img_cols = X_train.shape[1:3]\n",
    "#     img_channels = 3\n",
    "\n",
    "#     # Right shape for X\n",
    "#     X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols,\n",
    "#                               img_channels)\n",
    "#     X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "#     # Avoid more iterations once convergence\n",
    "#     patience_par = 10\n",
    "#     earlystopping = EarlyStopping(monitor='val_loss', patience=patience_par,\n",
    "#                                   verbose=0, mode='auto' )\n",
    "#     modelcheckpoint = ModelCheckpoint(output+\"_best.hd5\", monitor='val_loss',\n",
    "#                                       verbose=0, save_best_only=True)\n",
    "\n",
    "#     # Define CNN\n",
    "#     model = CNN(img_channels, img_rows, img_cols, verbose=True)\n",
    "\n",
    "#     if not data_augmentation:\n",
    "#         print('Not using data augmentation.')\n",
    "#         history = model.fit(X_train, Y_train,\n",
    "#                             batch_size=batch_size,\n",
    "#                             nb_epoch=nb_epoch,\n",
    "#                             validation_data=(X_val, Y_val),\n",
    "#                             shuffle=True, verbose=verbose,\n",
    "#                             callbacks=[earlystopping, modelcheckpoint])\n",
    "#     else:\n",
    "#         print('Using real-time data augmentation.')\n",
    "#         # this will do preprocessing and realtime data augmentation\n",
    "#         datagen = ImageDataGenerator(\n",
    "#             featurewise_center=False,\n",
    "#             samplewise_center=False,\n",
    "#             featurewise_std_normalization=False,\n",
    "#             samplewise_std_normalization=False,\n",
    "#             zca_whitening=False,\n",
    "#             rotation_range=45,\n",
    "#             width_shift_range=0.05,\n",
    "#             height_shift_range=0.05,\n",
    "#             horizontal_flip=True,\n",
    "#             vertical_flip=True,\n",
    "#             zoom_range=[0.75, 1.3])\n",
    "\n",
    "#         datagen.fit(X_train)\n",
    "\n",
    "#         history = model.fit_generator(\n",
    "#             datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "#             samples_per_epoch=X_train.shape[0],\n",
    "#             nb_epoch=nb_epoch,\n",
    "#             validation_data=(X_val, Y_val),\n",
    "#             callbacks=[earlystopping, modelcheckpoint])\n",
    "\n",
    "#     print(\"Saving model...\")\n",
    "#     # save weights\n",
    "#     model.save_weights(output+\".weights\", overwrite=True)\n",
    "\n",
    "\n",
    "# def apply_CNN(X, model_name):\n",
    "#     '''Apply a CNN to a data set'''\n",
    "#     # input image dimensions\n",
    "#     img_rows, img_cols = X.shape[1:3]\n",
    "#     img_channels = 3\n",
    "#     X = X.reshape(X.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "#     # load model & predict\n",
    "#     print(\"Loading weights\", model_name)\n",
    "\n",
    "#     model = CNN(img_channels, img_rows, img_cols)\n",
    "#     model.load_weights(model_name+\".weights\")\n",
    "#     Y_pred = model.predict_proba(X)\n",
    "\n",
    "#     return Y_pred\n",
    "\n",
    "\n",
    "# def add_titlebox(ax, text):\n",
    "#     '''Add an embedded title into figure panel'''\n",
    "#     ax.text(.1, .85, text,\n",
    "#             horizontalalignment='left',\n",
    "#             transform=ax.transAxes,\n",
    "#             bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# def plot_CNN_performance(pred, labels):\n",
    "#     '''Plot ROC curve and sample galaxies'''\n",
    "\n",
    "#     fig = plt.figure(figsize=(6, 3))\n",
    "#     fig.subplots_adjust(wspace=0.1, hspace=0.1,\n",
    "#                         left=0.1, right=0.95,\n",
    "#                         bottom=0.15, top=0.9)\n",
    "\n",
    "#     # define shape of figure\n",
    "#     gridsize = (2, 4)\n",
    "#     ax1 = plt.subplot2grid(gridsize, (0, 0), colspan=2, rowspan=2)\n",
    "#     ax2 = plt.subplot2grid(gridsize, (0, 2))\n",
    "#     ax3 = plt.subplot2grid(gridsize, (0, 3))\n",
    "#     ax4 = plt.subplot2grid(gridsize, (1, 2))\n",
    "#     ax5 = plt.subplot2grid(gridsize, (1, 3))\n",
    "\n",
    "#     # plot ROC curve\n",
    "#     fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "\n",
    "#     ax1.plot(fpr, tpr, color='black')\n",
    "#     ax1.set_xlabel(r'False Positive Rate')\n",
    "#     ax1.set_ylabel(r'True Positive Rate')\n",
    "\n",
    "#     # array of objects (good E, good S, bad E, bad S)\n",
    "#     goodE = np.where((pred[:, 0] < 0.5) & (labels == 0))\n",
    "#     goodS = np.where((pred[:, 0] > 0.5) & (labels == 1))\n",
    "#     badE = np.where((pred[:, 0] < 0.5) & (labels == 1))\n",
    "#     badS = np.where((pred[:, 0] > 0.5) & (labels == 0))\n",
    "\n",
    "#     ax2.imshow(D[pred_index + goodE[0][1]])\n",
    "#     add_titlebox(ax2, \"Correct E\")\n",
    "#     ax2.axis('off')\n",
    "\n",
    "#     ax3.imshow(D[pred_index + goodS[0][4]])\n",
    "#     add_titlebox(ax3, \"Correct Spiral\")\n",
    "#     ax3.axis('off')\n",
    "\n",
    "#     ax4.imshow(D[pred_index + badE[0][1]])\n",
    "#     add_titlebox(ax4, \"Incorrect E\")\n",
    "#     ax4.axis('off')\n",
    "\n",
    "#     ax5.imshow(D[pred_index + badS[0][3]])\n",
    "#     add_titlebox(ax5, \"Incorrect Spiral\")\n",
    "#     ax5.axis('off')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# n_objects = 500\n",
    "# save_files = \"./SDSS{}\".format(n_objects)\n",
    "\n",
    "# # Read SDSS images and labels\n",
    "# D = read_savefile(\"sdss_images_1000.npy\")[0:n_objects]\n",
    "# Y = read_savefile(\"sdss_labels_1000.npy\")[0:n_objects]\n",
    "\n",
    "# # Train network and output to disk (keep 10% of data for test set)\n",
    "# ntrain = D.shape[0] * 8 // 10.\n",
    "# nval = D.shape[0] // 10\n",
    "# npred = D.shape[0] - (ntrain + nval)  # test sample size;\n",
    "# pred_index = ntrain + nval            # test sample start index;\n",
    "\n",
    "# # Normalize images\n",
    "# mu = np.amax(D, axis=(1, 2))\n",
    "# for i in range(0, mu.shape[0]):\n",
    "#     D[i, :, :, 0] = D[i, :, :, 0] / mu[i, 0]\n",
    "#     D[i, :, :, 1] = D[i, :, :, 1] / mu[i, 1]\n",
    "#     D[i, :, :, 2] = D[i, :, :, 2] / mu[i, 2]\n",
    "\n",
    "# # change order so that we do not use always the same objects to train/test\n",
    "# D, Y, = shuffle(D, Y, random_state=0)\n",
    "\n",
    "# my_file = Path(save_files + \".weights\")\n",
    "# if my_file.is_file():\n",
    "#     Y_pred = apply_CNN(D[pred_index:pred_index + npred, :, :, :], save_files)\n",
    "#     Y_test=Y[pred_index:pred_index + npred]\n",
    "# else:\n",
    "#     print(\"Training Model\")\n",
    "#     print(\"====================\")\n",
    "#     model_name = train_CNN(D, Y, ntrain, nval, output=save_files)\n",
    "#     Y_pred = apply_CNN(D[pred_index:pred_index + npred, :, :, :], save_files)\n",
    "#     Y_test = Y[pred_index:pred_index + npred]\n",
    "\n",
    "# Y_pred_class = Y_pred * 0\n",
    "# Y_pred_class[Y_pred > 0.5] = 1\n",
    "# print(\"Global Accuracy:\", accuracy_score(Y_test, Y_pred_class))\n",
    "\n",
    "\n",
    "# plot_CNN_performance(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() #Make sure that we are starting a new model and not adding to an earlier one\n",
    "np.random.seed(42) #Set the numpy and tensorflow random seeds so that we all get the same answer\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2aa51d5fd3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Then 10% each for supervised training, validation, and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_full' is not defined"
     ]
    }
   ],
   "source": [
    "#We'll use 60% of the sample to do unsupervised training with an autoencoder\n",
    "#Then 10% each for supervised training, validation, and testing\n",
    "#(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-521b9bcac731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_rest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33333\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_full' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_A, X_rest, y_train_A, y_rest = train_test_split(X_train_full, y_train_full, test_size=0.33333, random_state=42)\n",
    "print(len(X_train_A),len(X_rest))\n",
    "\n",
    "X_train_B, X_valid, y_train_B, y_valid = train_test_split(X_rest, y_rest, test_size=0.50, random_state=42)\n",
    "print(len(X_train_B),len(X_valid))\n",
    "\n",
    "#Need to reshape for CNN since we have greyscale images\n",
    "X_train = X_train[:, :, :, np.newaxis]\n",
    "X_valid = X_valid[:,  :, :, np.newaxis]\n",
    "X_test = X_test[:, :, :, np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=7, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),  \n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the encoder and decoder\n",
    "stacked_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(200, activation=\"selu\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\"),\n",
    "])\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(200, activation=\"selu\", input_shape=[100]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
    "stacked_ae.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=keras.optimizers.SGD(lr=1.5), metrics=['accuracy'])\n",
    "history = stacked_ae.fit(X_train_A, X_train_A, epochs=50,\n",
    "                         validation_data=(X_valid, X_valid))\n",
    "\n",
    "#print score\n",
    "score = stacked_ae.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "#save the model\n",
    "stacked_ae.save(\"autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "#try learning rates between 5 and 50?\n",
    "tsne = TSNE(n_components=2,learning_rate=200)\n",
    "X_reduced = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=y, cmap=\"nipy_spectral\", edgecolor=\"None\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    #plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find optimal number of clusters for DBSCAN\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "range_n_clusters = range(1,len(X_data))\n",
    "\n",
    "best_n_clusters, best_score = 0,0\n",
    " \n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X_data)\n",
    "    \n",
    "    #silhouette score gives info on density and separation of formed clusters\n",
    "    silhouette_avg = silhouette_score(X_data, cluster_labels)\n",
    "    \n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_n_clusters = n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best hyperparameters for DBSCAN (min_samples and eps)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#assume min_samples = best_n_clusters to find optimal eps value\n",
    "range_eps = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "best_eps, best_silhouette_score = 0, 0\n",
    "for eps in range_eps:\n",
    "    db = DBSCAN(eps=eps, min_samples=best_n_clusters).fit(X_data)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    silhouette_avg = silhouette_score(X_data, labels)\n",
    "    \n",
    "    #highest silhouette score gives us best eps value\n",
    "    if silhouette_avg > best_silhouette_score:\n",
    "        best_silhouette_score = silhouette_avg\n",
    "        best_eps = eps\n",
    "    \n",
    "#now try various min_samples using this best_eps\n",
    "min_samples = range(1, 10)\n",
    "best_min_samples = 0\n",
    "for n_samples in min_samples:\n",
    "    db = DBSCAN(eps=best_eps, min_samples=n_samples).fit(X_data)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    \n",
    "    #ignore label -1 since that is outliers\n",
    "    labels = set([label for label in db.labels_ if label >= 0])\n",
    "\n",
    "    #if len of set(labels) == best_n_clusters, min_samples value is good\n",
    "    if len(set(labels)) == best_n_clusters:\n",
    "        best_min_samples = n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute DBSCAN and plot\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_min_samples).fit(X_data)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_dbscan(db, X_data, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
